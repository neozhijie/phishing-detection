{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "887e724a-94fb-4e26-a958-3f6508bd21c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weili\\AppData\\Local\\Temp\\ipykernel_13592\\3735331367.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfffbf90-add9-4011-82e2-9db029fd9b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df = pd.read_csv('dataset_phishing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77671735-877f-4e9e-8c93-0e90f9420d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['shortest_word_path',\n",
    " 'ratio_intMedia',\n",
    " 'links_in_tags',\n",
    " 'nb_hyphens',\n",
    " 'page_rank',\n",
    " 'avg_word_path',\n",
    " 'ratio_extHyperlinks',\n",
    " 'longest_words_raw',\n",
    " 'google_index',\n",
    " 'length_hostname',\n",
    " 'longest_word_host',\n",
    " 'domain_registration_length',\n",
    " 'nb_www',\n",
    " 'nb_underscore',\n",
    " 'nb_dots',\n",
    " 'ratio_extMedia',\n",
    " 'phish_hints',\n",
    " 'domain_in_title',\n",
    " 'web_traffic',\n",
    " 'safe_anchor',\n",
    " 'nb_space',\n",
    " 'shortening_service',\n",
    " 'ip',\n",
    " 'domain_age',\n",
    " 'nb_qm',\n",
    " 'nb_hyperlinks',\n",
    " 'nb_slash']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c83b0ac-9b8e-4cfe-b715-4002ae2437db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[features]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df[\"status\"])\n",
    "\n",
    "# Step 1: Split data into 70% trian and 30% temp (validation + test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 2: Split the temp set into 50% validation and 50% test (15% each of the original data)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2ad44d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "\n",
      "Validation Results:\n",
      "Accuracy: 0.9405\n",
      "Precision: 0.9388\n",
      "Recall: 0.9388\n",
      "F1 Score: 0.9388\n",
      "ROC-AUC Score: 0.9855\n",
      "Confusion Matrix:\n",
      "[[829  51]\n",
      " [ 51 783]]\n",
      "\n",
      "Final Test Results:\n",
      "Accuracy: 0.9399\n",
      "Precision: 0.9429\n",
      "Recall: 0.9374\n",
      "F1 Score: 0.9402\n",
      "ROC-AUC Score: 0.9820\n",
      "Confusion Matrix:\n",
      "[[803  49]\n",
      " [ 54 809]]\n"
     ]
    }
   ],
   "source": [
    "# Define the Logistic Regression model and a hyperparameter grid for tuning\n",
    "logreg = LogisticRegression(max_iter=1000)  # Default Logistic Regression\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],  # Regularization parameter\n",
    "    'solver': ['liblinear', 'lbfgs']  # Solvers\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV with validation set\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='recall', verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_logreg = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate on the validation set\n",
    "y_val_pred = best_logreg.predict(X_val)\n",
    "y_val_pred_prob = best_logreg.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Calculate validation metrics\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_precision = precision_score(y_val, y_val_pred)\n",
    "val_recall = recall_score(y_val, y_val_pred)\n",
    "val_f1 = f1_score(y_val, y_val_pred)\n",
    "val_roc_auc = roc_auc_score(y_val, y_val_pred_prob)\n",
    "val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "# Display validation results\n",
    "print(\"\\nValidation Results:\")\n",
    "print(f\"Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Precision: {val_precision:.4f}\")\n",
    "print(f\"Recall: {val_recall:.4f}\")\n",
    "print(f\"F1 Score: {val_f1:.4f}\")\n",
    "print(f\"ROC-AUC Score: {val_roc_auc:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{val_conf_matrix}\")\n",
    "\n",
    "# Final evaluation on the test set\n",
    "y_test_pred = best_logreg.predict(X_test)\n",
    "y_test_pred_prob = best_logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate test metrics\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "test_roc_auc = roc_auc_score(y_test, y_test_pred_prob)\n",
    "test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Display final test results\n",
    "print(\"\\nFinal Test Results:\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall: {test_recall:.4f}\")\n",
    "print(f\"F1 Score: {test_f1:.4f}\")\n",
    "print(f\"ROC-AUC Score: {test_roc_auc:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{test_conf_matrix}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbed1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-conda-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
