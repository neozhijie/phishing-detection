{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1c5c96-f6e8-4438-bc0e-7e1ef9fa0ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pandas numpy matplotlib seaborn scikit-learn tensorflow keras-tuner\n",
    "#pip install scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc959130-77ce-468c-ac9a-051cca1f7200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM, Activation, Dropout\n",
    "from keras_tuner.tuners import GridSearch\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix)\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import (Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ee2c5b4-b351-4308-bc9b-52b6a9ce52ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df = pd.read_csv('dataset_phishing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d913e896-6b00-4d1d-9f1f-8357641f78eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['shortest_word_path', 'ratio_intMedia', 'links_in_tags', 'nb_hyphens', 'page_rank', 'avg_word_path', 'ratio_extHyperlinks',\n",
    " 'longest_words_raw', 'google_index', 'length_hostname', 'longest_word_host', 'domain_registration_length', 'nb_www', 'nb_underscore', 'nb_dots',\n",
    " 'ratio_extMedia', 'phish_hints', 'domain_in_title', 'web_traffic', 'safe_anchor', 'nb_space', 'shortening_service', 'ip', 'domain_age', 'nb_qm',\n",
    " 'nb_hyperlinks', 'nb_slash']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c67c27c-345b-44bb-bba0-f484b1938ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[features]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df[\"status\"])\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1145e22a-a298-4f8a-ae6c-1faaf9d70b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    }
   ],
   "source": [
    "# Handle class imbalance by oversampling the minority class (phishing)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define the MLPClassifier with early stopping to avoid overfitting\n",
    "mlp = MLPClassifier(random_state=42, early_stopping=True, validation_fraction=0.15)\n",
    "\n",
    "# Set up hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(200, 100)],\n",
    "    'activation': ['relu', 'tanh', 'logistic'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001],\n",
    "    'learning_rate': ['constant', 'adaptive'],\n",
    "    'learning_rate_init': [0.008],\n",
    "}\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=5, scoring='recall', n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_mlp = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "y_val_pred = best_mlp.predict(X_val)\n",
    "\n",
    "# Calculate performance metrics for the validation set\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_precision = precision_score(y_val, y_val_pred)\n",
    "val_recall = recall_score(y_val, y_val_pred)\n",
    "val_f1 = f1_score(y_val, y_val_pred)\n",
    "val_roc_auc = roc_auc_score(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7db326b2-6139-4190-96f8-56b9ed934b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a seed value for reproducibility\n",
    "seed = 42\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "def create_model(filters_1=32, kernel_size_1=3, dropout_rate_1=0.2,\n",
    "                 filters_2=64, kernel_size_2=3, dropout_rate_2=0.2,\n",
    "                 dense_units=128, learning_rate=0.001):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First Conv1D layer\n",
    "    model.add(Conv1D(filters=filters_1, kernel_size=kernel_size_1,\n",
    "                     activation='relu', input_shape=(X_train_cnn.shape[1], 1),\n",
    "                     padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(dropout_rate_1))\n",
    "    \n",
    "    # Second Conv1D layer\n",
    "    model.add(Conv1D(filters=filters_2, kernel_size=kernel_size_2,\n",
    "                     activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(dropout_rate_2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(dense_units, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy', tf.keras.metrics.Recall()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5972904f-435d-4a52-81b9-ad073a0a49fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ss/kwxwfq350cqd6j60p6ssx2xc0000gn/T/ipykernel_93084/369168224.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.fillna(0, inplace=True)\n",
      "/Users/teckyew/anaconda3/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training final model with best parameters...\n",
      "Epoch 1/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8991 - loss: 0.2734 - recall: 0.8860 - val_accuracy: 0.8856 - val_loss: 0.2879 - val_recall: 0.7865\n",
      "Epoch 2/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9315 - loss: 0.1915 - recall: 0.9215 - val_accuracy: 0.9405 - val_loss: 0.1607 - val_recall: 0.9382\n",
      "Epoch 3/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9368 - loss: 0.1750 - recall: 0.9317 - val_accuracy: 0.9376 - val_loss: 0.1687 - val_recall: 0.9475\n",
      "Epoch 4/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9396 - loss: 0.1720 - recall: 0.9404 - val_accuracy: 0.9434 - val_loss: 0.1510 - val_recall: 0.9347\n",
      "Epoch 5/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9416 - loss: 0.1563 - recall: 0.9395 - val_accuracy: 0.9422 - val_loss: 0.1519 - val_recall: 0.9405\n",
      "Epoch 6/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9435 - loss: 0.1500 - recall: 0.9404 - val_accuracy: 0.9481 - val_loss: 0.1508 - val_recall: 0.9440\n",
      "Epoch 7/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9505 - loss: 0.1370 - recall: 0.9449 - val_accuracy: 0.9422 - val_loss: 0.1583 - val_recall: 0.9300\n",
      "Epoch 8/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9481 - loss: 0.1371 - recall: 0.9467 - val_accuracy: 0.9440 - val_loss: 0.1543 - val_recall: 0.9323\n"
     ]
    }
   ],
   "source": [
    "X = df[features]\n",
    "y = df['status']\n",
    "\n",
    "# Map the 'status' to numerical labels\n",
    "label_mapping = {'legitimate': 0, 'phishing': 1}\n",
    "y = y.map(label_mapping)\n",
    "\n",
    "# Handle missing values in features if any\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "# Reshape the data\n",
    "X_reshaped = X.values\n",
    "\n",
    "# Perform the split as specified\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_reshaped, y, test_size=0.3, random_state=seed, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=seed, stratify=y_temp)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape for CNN\n",
    "X_train_cnn = X_train_scaled.reshape(-1, X_train_scaled.shape[1], 1)\n",
    "X_val_cnn = X_val_scaled.reshape(-1, X_val_scaled.shape[1], 1)\n",
    "X_test_cnn = X_test_scaled.reshape(-1, X_test_scaled.shape[1], 1)\n",
    "\n",
    "\n",
    "best_params = {'dense_units': 64, 'dropout_rate_1': 0.2, 'dropout_rate_2': 0.2, 'filters_1': 32, 'filters_2': 64, 'kernel_size_1': 3, 'kernel_size_2': 2, 'learning_rate': 0.01}\n",
    "\n",
    "# Create and train the model with best parameters\n",
    "print(\"\\nTraining final model with best parameters...\")\n",
    "best_cnn = create_model(**best_params)\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_recall', patience=5, \n",
    "                             restore_best_weights=True, mode='max')\n",
    "\n",
    "# Train the model\n",
    "history = best_cnn.fit(\n",
    "    X_train_cnn, y_train,\n",
    "    epochs=50,\n",
    "    validation_data=(X_val_cnn, y_val),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae0e22c5-2fbe-4c4c-963d-7a337b67a7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/teckyew/anaconda3/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.9109 - loss: 0.3214 - val_accuracy: 0.9452 - val_loss: 0.1545\n",
      "Epoch 2/5\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.9444 - loss: 0.1454 - val_accuracy: 0.9463 - val_loss: 0.1436\n",
      "Epoch 3/5\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.9518 - loss: 0.1277 - val_accuracy: 0.9510 - val_loss: 0.1370\n",
      "Epoch 4/5\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.9555 - loss: 0.1169 - val_accuracy: 0.9516 - val_loss: 0.1337\n",
      "Epoch 5/5\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.9589 - loss: 0.1100 - val_accuracy: 0.9539 - val_loss: 0.1308\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Accuracy: 0.9650\n",
      "Precision: 0.9672\n",
      "Recall:  0.9627\n",
      "F1 Score: 0.9649\n",
      "ROC_AUC Score: 0.9650\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.97       858\n",
      "           1       0.97      0.96      0.96       857\n",
      "\n",
      "    accuracy                           0.97      1715\n",
      "   macro avg       0.97      0.97      0.97      1715\n",
      "weighted avg       0.97      0.97      0.97      1715\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#LSTM\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "#Reshape features with 1 timestep to fit into RNN\n",
    "X_reshaped = X_scaled.reshape((X.shape[0],1,X.shape[1]))  \n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df[\"status\"])\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_reshaped, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "#Define model to fir into GridSearch later\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hp.Choice('units',[128,512,1024,2048]), activation=hp.Choice('activation',['relu','tanh']),input_shape=(1,X.shape[1]))) #4 Different layer sizes\n",
    "    model.add(Dropout(hp.Float('dropout',0.1,0.5,step=0.1))) #5 different dropout values\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "best_lstm = Sequential()\n",
    "best_lstm.add(LSTM(1024, activation='relu',input_shape=(1,X.shape[1]))) \n",
    "best_lstm.add(Dropout(0.1))\n",
    "best_lstm.add(Dense(1, activation='sigmoid'))\n",
    "best_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = best_lstm.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_val,y_val))\n",
    "\n",
    "y_pred = (best_lstm.predict(X_test) > 0.5)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test,y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test,y_pred):.4f}\")\n",
    "print(f\"Recall:  {recall_score(y_test,y_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test,y_pred):.4f}\")\n",
    "print(f\"ROC_AUC Score: {roc_auc_score(y_test,y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ece47f6-8d4d-4bef-ab70-68f0b2a6f632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of ensemble model (Stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcbd3ee8-6d5d-4984-8407-8012599436da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 595us/step\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\n",
      "Performance Metrics for Stacked Model on Validation Set:\n",
      "Accuracy       : 0.9516\n",
      "Precision      : 0.9397\n",
      "Recall         : 0.9631\n",
      "F1 Score       : 0.9513\n",
      "AUC            : 0.9896\n",
      "\n",
      "Performance Metrics for Stacked Model on Test Set:\n",
      "Accuracy       : 0.9598\n",
      "Precision      : 0.9589\n",
      "Recall         : 0.9622\n",
      "F1 Score       : 0.9605\n",
      "AUC            : 0.9900\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, roc_auc_score)\n",
    "\n",
    "# Load your trained models\n",
    "mlp_model = best_mlp  # Use your trained MLP model\n",
    "cnn_model = best_cnn\n",
    "lstm_model = best_lstm\n",
    "\n",
    "# Read data\n",
    "df = pd.read_csv('dataset_phishing.csv')\n",
    "features = ['shortest_word_path', 'ratio_intMedia', 'links_in_tags', 'nb_hyphens', \n",
    "            'page_rank', 'avg_word_path', 'ratio_extHyperlinks', 'longest_words_raw', \n",
    "            'google_index', 'length_hostname', 'longest_word_host', \n",
    "            'domain_registration_length', 'nb_www', 'nb_underscore', \n",
    "            'nb_dots', 'ratio_extMedia', 'phish_hints', \n",
    "            'domain_in_title', 'web_traffic', 'safe_anchor', \n",
    "            'nb_space', 'shortening_service', 'ip', 'domain_age', \n",
    "            'nb_qm', 'nb_hyperlinks', 'nb_slash']\n",
    "\n",
    "X = df[features]\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Encode target variable\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df[\"status\"])\n",
    "\n",
    "# Split data into training, validation, and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Reshape data for CNN and LSTM\n",
    "X_train_cnn = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_val_cnn = np.reshape(X_val, (X_val.shape[0], X_val.shape[1], 1))\n",
    "X_test_cnn = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "X_train_lstm = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_val_lstm = np.reshape(X_val, (X_val.shape[0], 1, X_val.shape[1]))  \n",
    "X_test_lstm = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Get predictions from each model\n",
    "mlp_predictions_train = mlp_model.predict(X_train)\n",
    "mlp_predictions_val = mlp_model.predict(X_val)\n",
    "mlp_predictions_test = mlp_model.predict(X_test)\n",
    "\n",
    "cnn_predictions_train = cnn_model.predict(X_train_cnn)\n",
    "cnn_predictions_val = cnn_model.predict(X_val_cnn)\n",
    "cnn_predictions_test = cnn_model.predict(X_test_cnn)\n",
    "\n",
    "lstm_predictions_train = lstm_model.predict(X_train_lstm)\n",
    "lstm_predictions_val = lstm_model.predict(X_val_lstm)\n",
    "lstm_predictions_test = lstm_model.predict(X_test_lstm)\n",
    "\n",
    "# Stack predictions into a new feature set for the final estimator\n",
    "X_train_stacked = np.column_stack((mlp_predictions_train, cnn_predictions_train, lstm_predictions_train))\n",
    "X_val_stacked = np.column_stack((mlp_predictions_val, cnn_predictions_val, lstm_predictions_val))\n",
    "X_test_stacked = np.column_stack((mlp_predictions_test, cnn_predictions_test, lstm_predictions_test))\n",
    "\n",
    "# Train a final estimator using the stacked predictions\n",
    "final_estimator = LogisticRegression(random_state=42)\n",
    "\n",
    "# Fit the final estimator\n",
    "final_estimator.fit(X_train_stacked, y_train)\n",
    "\n",
    "# Make predictions on the validation and test sets\n",
    "stacked_predictions_val = final_estimator.predict(X_val_stacked)\n",
    "stacked_predictions_test = final_estimator.predict(X_test_stacked)\n",
    "\n",
    "# Calculate performance metrics on the validation set\n",
    "accuracy_val = accuracy_score(y_val, stacked_predictions_val)\n",
    "precision_val = precision_score(y_val, stacked_predictions_val)\n",
    "recall_val = recall_score(y_val, stacked_predictions_val)\n",
    "f1_val = f1_score(y_val, stacked_predictions_val)\n",
    "stacked_probabilities_val = final_estimator.predict_proba(X_val_stacked)\n",
    "auc_val = roc_auc_score(y_val, stacked_probabilities_val[:, 1])\n",
    "\n",
    "# Calculate performance metrics on the test set\n",
    "accuracy_test = accuracy_score(y_test, stacked_predictions_test)\n",
    "precision_test = precision_score(y_test, stacked_predictions_test)\n",
    "recall_test = recall_score(y_test, stacked_predictions_test)\n",
    "f1_test = f1_score(y_test, stacked_predictions_test)\n",
    "stacked_probabilities_test = final_estimator.predict_proba(X_test_stacked)\n",
    "auc_test = roc_auc_score(y_test, stacked_probabilities_test[:, 1])\n",
    "\n",
    "# Print performance metrics\n",
    "print(\"\\nPerformance Metrics for Stacked Model on Validation Set:\")\n",
    "print(f\"Accuracy       : {accuracy_val:.4f}\")\n",
    "print(f\"Precision      : {precision_val:.4f}\")\n",
    "print(f\"Recall         : {recall_val:.4f}\")\n",
    "print(f\"F1 Score       : {f1_val:.4f}\")\n",
    "print(f\"AUC            : {auc_val:.4f}\")\n",
    "\n",
    "print(\"\\nPerformance Metrics for Stacked Model on Test Set:\")\n",
    "print(f\"Accuracy       : {accuracy_test:.4f}\")\n",
    "print(f\"Precision      : {precision_test:.4f}\")\n",
    "print(f\"Recall         : {recall_test:.4f}\")\n",
    "print(f\"F1 Score       : {f1_test:.4f}\")\n",
    "print(f\"AUC            : {auc_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367e28ea-4fb3-4425-970b-27a72760c20f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
