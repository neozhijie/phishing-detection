{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23ee7101-46ca-4da6-93d9-09d31fefbbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix)\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Read data\n",
    "df = pd.read_csv('dataset_phishing.csv')\n",
    "features = ['shortest_word_path','ratio_intMedia','links_in_tags','nb_hyphens','page_rank','avg_word_path',\n",
    " 'ratio_extHyperlinks','longest_words_raw','google_index','length_hostname','longest_word_host','domain_registration_length',\n",
    " 'nb_www','nb_underscore','nb_dots','ratio_extMedia','phish_hints','domain_in_title','web_traffic','safe_anchor',\n",
    " 'nb_space','shortening_service','ip','domain_age','nb_qm','nb_hyperlinks','nb_slash']\n",
    "\n",
    "X = df[features]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df[\"status\"])\n",
    "\n",
    "# Step 1: Split data into 70% train and 30% temp (validation + test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Step 2: Split the temp set into 50% validation and 50% test (15% each of the original data)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c339b6-aa9c-4e10-b3db-f81bd0e85ddc",
   "metadata": {},
   "source": [
    "# Traditional Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1732ec00-29a2-4561-b7bd-4503828d7881",
   "metadata": {},
   "source": [
    "## Traditional Method 1: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3489464a-d1f6-47fa-ab7a-51d0e5a22bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for SVM: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "SVM Validation Results:\n",
      "Validation Accuracy: 0.9574095682613769\n",
      "Validation Precision: 0.951764705882353\n",
      "Validation Recall: 0.9619500594530321\n",
      "Validation F1 Score: 0.9568302779420461\n",
      "Confusion Matrix:\n",
      " [[813  29]\n",
      " [ 38 835]]\n",
      "Accuracy: 0.960932944606414\n",
      "Precision: 0.9664351851851852\n",
      "Recall: 0.9564719358533792\n",
      "F1 Score: 0.9614277489925158\n"
     ]
    }
   ],
   "source": [
    "# ======================================= METHOD 1: SVM ================================\n",
    "# Hyperparameter tuning for SVM\n",
    "svm_param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "svm_grid_search = GridSearchCV(SVC(probability=True, random_state=42), svm_param_grid, cv=5)\n",
    "svm_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters for SVM\n",
    "print(\"Best parameters for SVM:\", svm_grid_search.best_params_)\n",
    "\n",
    "# Validate and test the best SVM model\n",
    "svm_model = svm_grid_search.best_estimator_\n",
    "y_val_pred_svm = svm_model.predict(X_val)\n",
    "\n",
    "# Compute metrics for validation set\n",
    "svm_val_accuracy = accuracy_score(y_val, y_val_pred_svm)\n",
    "svm_val_precision = precision_score(y_val, y_val_pred_svm)\n",
    "svm_val_recall = recall_score(y_val, y_val_pred_svm)\n",
    "svm_val_f1_score = f1_score(y_val, y_val_pred_svm)\n",
    "\n",
    "# Print validation results\n",
    "print(\"SVM Validation Results:\")\n",
    "print(\"Validation Accuracy:\", svm_val_accuracy)\n",
    "print(\"Validation Precision:\", svm_val_precision)\n",
    "print(\"Validation Recall:\", svm_val_recall)\n",
    "print(\"Validation F1 Score:\", svm_val_f1_score)\n",
    "\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "svm_conf_matrix = confusion_matrix(y_test, y_pred_svm)\n",
    "\n",
    "# Compute metrics for SVM\n",
    "svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
    "svm_precision = precision_score(y_test, y_pred_svm)\n",
    "svm_recall = recall_score(y_test, y_pred_svm)\n",
    "svm_f1_score = f1_score(y_test, y_pred_svm)\n",
    "\n",
    "# Print SVM results\n",
    "print(\"Confusion Matrix:\\n\", svm_conf_matrix)\n",
    "print(\"Accuracy:\", svm_accuracy)\n",
    "print(\"Precision:\", svm_precision)\n",
    "print(\"Recall:\", svm_recall)\n",
    "print(\"F1 Score:\", svm_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba2921f-c524-4909-b7c6-7b582df53ace",
   "metadata": {},
   "source": [
    "## Traditional Method 2: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78e90d28-a8a6-449c-9b46-a67b3756b283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Decision Tree: {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Decision Tree Validation Results:\n",
      "Validation Accuracy: 0.9323220536756126\n",
      "Validation Precision: 0.9239766081871345\n",
      "Validation Recall: 0.93935790725327\n",
      "Validation F1 Score: 0.9316037735849056\n",
      "Decision Tree Test Results:\n",
      "Confusion Matrix:\n",
      " [[796  46]\n",
      " [ 42 831]]\n",
      "Accuracy: 0.9486880466472303\n",
      "Precision: 0.9475484606613455\n",
      "Recall: 0.9518900343642611\n",
      "F1 Score: 0.9497142857142857\n"
     ]
    }
   ],
   "source": [
    "# ================================ METHOD 2: Traditional Tree ==================\n",
    "\n",
    "# Hyperparameter tuning for Decision Tree\n",
    "dt_param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "dt_grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), dt_param_grid, cv=5)\n",
    "dt_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters for Decision Tree\n",
    "print(\"Best parameters for Decision Tree:\", dt_grid_search.best_params_)\n",
    "\n",
    "# Validate and test the best Decision Tree model\n",
    "dt_model = dt_grid_search.best_estimator_\n",
    "y_val_pred_dt = dt_model.predict(X_val)\n",
    "\n",
    "# Compute metrics for validation set\n",
    "dt_val_accuracy = accuracy_score(y_val, y_val_pred_dt)\n",
    "dt_val_precision = precision_score(y_val, y_val_pred_dt)\n",
    "dt_val_recall = recall_score(y_val, y_val_pred_dt)\n",
    "dt_val_f1_score = f1_score(y_val, y_val_pred_dt)\n",
    "\n",
    "# Print validation results\n",
    "print(\"Decision Tree Validation Results:\")\n",
    "print(\"Validation Accuracy:\", dt_val_accuracy)\n",
    "print(\"Validation Precision:\", dt_val_precision)\n",
    "print(\"Validation Recall:\", dt_val_recall)\n",
    "print(\"Validation F1 Score:\", dt_val_f1_score)\n",
    "\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "dt_conf_matrix = confusion_matrix(y_test, y_pred_dt)\n",
    "\n",
    "# Compute metrics for Decision Tree\n",
    "dt_accuracy = accuracy_score(y_test, y_pred_dt)\n",
    "dt_precision = precision_score(y_test, y_pred_dt)\n",
    "dt_recall = recall_score(y_test, y_pred_dt)\n",
    "dt_f1_score = f1_score(y_test, y_pred_dt)\n",
    "\n",
    "# Print Decision Tree results\n",
    "print(\"Decision Tree Test Results:\")\n",
    "print(\"Confusion Matrix:\\n\", dt_conf_matrix)\n",
    "print(\"Accuracy:\", dt_accuracy)\n",
    "print(\"Precision:\", dt_precision)\n",
    "print(\"Recall:\", dt_recall)\n",
    "print(\"F1 Score:\", dt_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bb3c64-e42b-47ce-915f-c1bfbaca011e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Traditional Method 3: KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8a45576-39f8-4c60-bc7e-834fbb3c5b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 240 candidates, totalling 1200 fits\n",
      "KNeighborsClassifier(metric='manhattan', n_neighbors=16, p=1,\n",
      "                     weights='distance')\n",
      "\n",
      "Validation Results:\n",
      "Accuracy: 0.9516\n",
      "Precision: 0.9397\n",
      "Recall: 0.9631\n",
      "F1 Score: 0.9513\n",
      "Confusion Matrix:\n",
      "[[821  52]\n",
      " [ 31 810]]\n",
      "\n",
      "Final Test Results:\n",
      "Accuracy: 0.9609\n",
      "Precision: 0.9664\n",
      "Recall: 0.9565\n",
      "F1 Score: 0.9614\n",
      "Confusion Matrix:\n",
      "[[813  29]\n",
      " [ 33 840]]\n"
     ]
    }
   ],
   "source": [
    "# Define KNN model and hyperparameter grid\n",
    "knn = KNeighborsClassifier()\n",
    "param_grid = {\n",
    "    'n_neighbors': range(1, 21),           # Test k values from 1 to 20\n",
    "    'weights': ['uniform', 'distance'],    # Uniform or distance-weighted voting\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski'],  # Different distance metrics\n",
    "    'p': [1, 2]                            # Power parameter for Minkowski (p=1 is Manhattan, p=2 is Euclidean)\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV with validation set\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='recall', verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_knn = grid_search.best_estimator_\n",
    "print(best_knn)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "y_val_pred = best_knn.predict(X_val)\n",
    "y_val_pred_prob = best_knn.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Calculate validation metrics\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_precision = precision_score(y_val, y_val_pred)\n",
    "val_recall = recall_score(y_val, y_val_pred)\n",
    "val_f1 = f1_score(y_val, y_val_pred)\n",
    "val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "# Display validation results\n",
    "print(\"\\nValidation Results:\")\n",
    "print(f\"Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Precision: {val_precision:.4f}\")\n",
    "print(f\"Recall: {val_recall:.4f}\")\n",
    "print(f\"F1 Score: {val_f1:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{val_conf_matrix}\")\n",
    "\n",
    "# Final evaluation on the test set\n",
    "y_test_pred = best_knn.predict(X_test)\n",
    "\n",
    "# Calculate test metrics\n",
    "test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_svm)\n",
    "test_precision = precision_score(y_test, y_pred_svm)\n",
    "test_recall = recall_score(y_test, y_pred_svm)\n",
    "test_f1 = f1_score(y_test, y_pred_svm)\n",
    "\n",
    "# Display final test results\n",
    "print(\"\\nFinal Test Results:\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall: {test_recall:.4f}\")\n",
    "print(f\"F1 Score: {test_f1:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{test_conf_matrix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d34a08-0df1-44b4-95b9-404757ed2eb0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Traditional Method 4: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "722c342d-1283-4459-a4ce-2a7a0eb37239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "LogisticRegression(C=100, max_iter=1000, solver='liblinear')\n",
      "\n",
      "Validation Results:\n",
      "Accuracy: 0.9347\n",
      "Precision: 0.9224\n",
      "Recall: 0.9465\n",
      "F1 Score: 0.9343\n",
      "Confusion Matrix:\n",
      "[[806  67]\n",
      " [ 45 796]]\n",
      "\n",
      "Final Test Results:\n",
      "Accuracy: 0.9388\n",
      "Precision: 0.9486\n",
      "Recall: 0.9301\n",
      "F1 Score: 0.9393\n",
      "Confusion Matrix:\n",
      "[[798  44]\n",
      " [ 61 812]]\n"
     ]
    }
   ],
   "source": [
    "# Define the Logistic Regression model and a hyperparameter grid for tuning\n",
    "logreg = LogisticRegression(max_iter=1000)  # Default Logistic Regression\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],  # Regularization parameter\n",
    "    'solver': ['liblinear', 'lbfgs']  # Solvers\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV with validation set\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='recall', verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_logreg = grid_search.best_estimator_\n",
    "print(best_logreg)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "y_val_pred = best_logreg.predict(X_val)\n",
    "y_val_pred_prob = best_logreg.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Calculate validation metrics\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_precision = precision_score(y_val, y_val_pred)\n",
    "val_recall = recall_score(y_val, y_val_pred)\n",
    "val_f1 = f1_score(y_val, y_val_pred)\n",
    "val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "# Display validation results\n",
    "print(\"\\nValidation Results:\")\n",
    "print(f\"Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Precision: {val_precision:.4f}\")\n",
    "print(f\"Recall: {val_recall:.4f}\")\n",
    "print(f\"F1 Score: {val_f1:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{val_conf_matrix}\")\n",
    "\n",
    "# Final evaluation on the test set\n",
    "y_test_pred = best_logreg.predict(X_test)\n",
    "y_test_pred_prob = best_logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate test metrics\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Display final test results\n",
    "print(\"\\nFinal Test Results:\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall: {test_recall:.4f}\")\n",
    "print(f\"F1 Score: {test_f1:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{test_conf_matrix}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f1dc19-80ed-4c7a-9991-b9b95777ed0f",
   "metadata": {},
   "source": [
    "## Traditional Method 5: Naives Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36381e8d-6b64-4a53-bd39-929140efe034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Grid Search on GaussianNB with validation set...\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "GaussianNB()\n",
      "Performing Grid Search on BernoulliNB with validation set...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "BernoulliNB(alpha=0.5)\n",
      "\n",
      "Final Test Results for GaussianNB:\n",
      "Accuracy: 0.8880\n",
      "Precision: 0.9316\n",
      "Recall: 0.8419\n",
      "F1 Score: 0.8845\n",
      "Confusion Matrix:\n",
      "[[788  54]\n",
      " [138 735]]\n",
      "\n",
      "Final Test Results for BernoulliNB:\n",
      "Accuracy: 0.9213\n",
      "Precision: 0.9222\n",
      "Recall: 0.9233\n",
      "F1 Score: 0.9227\n",
      "Confusion Matrix:\n",
      "[[774  68]\n",
      " [ 67 806]]\n"
     ]
    }
   ],
   "source": [
    "# Define the models and hyperparameter grids\n",
    "models = {\n",
    "    'GaussianNB': (GaussianNB(), {'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]}),\n",
    "    'BernoulliNB': (BernoulliNB(), {'alpha': [0.5, 1.0, 1.5, 2.0], 'binarize': [0.0, 0.5, 1.0]})\n",
    "}\n",
    "\n",
    "# Initialize results dictionary\n",
    "results = {}\n",
    "\n",
    "# Perform GridSearchCV and evaluate model on validation set function\n",
    "def grid_search_model(model, param_grid, X_train, X_val, y_train, y_val):\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='recall', verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(best_model)\n",
    "    y_val_pred = best_model.predict(X_val)\n",
    "    \n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    precision = precision_score(y_val, y_val_pred)\n",
    "    recall = recall_score(y_val, y_val_pred)\n",
    "    f1 = f1_score(y_val, y_val_pred)\n",
    "    conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "    return {\n",
    "        'best_model': best_model,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "# Perform GridSearchCV and validate each model\n",
    "for model_name, (model, param_grid) in models.items():\n",
    "    print(f\"Performing Grid Search on {model_name} with validation set...\")\n",
    "    results[model_name] = grid_search_model(model, param_grid, X_train, X_val, y_train, y_val)\n",
    "\n",
    "# Test the best models on the test set\n",
    "for model_name, metrics in results.items():\n",
    "    best_model = metrics['best_model']\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    precision = precision_score(y_test, y_test_pred)\n",
    "    recall = recall_score(y_test, y_test_pred)\n",
    "    f1 = f1_score(y_test, y_test_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "    # Display final results for the test set\n",
    "    print(f\"\\nFinal Test Results for {model_name}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{conf_matrix}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
