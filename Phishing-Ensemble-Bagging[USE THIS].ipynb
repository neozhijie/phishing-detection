{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23ee7101-46ca-4da6-93d9-09d31fefbbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix)\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Read data\n",
    "df = pd.read_csv('dataset_phishing.csv')\n",
    "features = ['shortest_word_path','ratio_intMedia','links_in_tags','nb_hyphens','page_rank','avg_word_path',\n",
    " 'ratio_extHyperlinks','longest_words_raw','google_index','length_hostname','longest_word_host','domain_registration_length',\n",
    " 'nb_www','nb_underscore','nb_dots','ratio_extMedia','phish_hints','domain_in_title','web_traffic','safe_anchor',\n",
    " 'nb_space','shortening_service','ip','domain_age','nb_qm','nb_hyperlinks','nb_slash']\n",
    "\n",
    "X = df[features]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df[\"status\"])\n",
    "\n",
    "# Step 1: Split data into 70% train and 30% temp (validation + test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Step 2: Split the temp set into 50% validation and 50% test (15% each of the original data)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c339b6-aa9c-4e10-b3db-f81bd0e85ddc",
   "metadata": {},
   "source": [
    "# Traditional Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1732ec00-29a2-4561-b7bd-4503828d7881",
   "metadata": {},
   "source": [
    "## Traditional Method 1: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3489464a-d1f6-47fa-ab7a-51d0e5a22bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for SVM: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "SVM Validation Results:\n",
      "Validation Accuracy: 0.9574095682613769\n",
      "Validation Precision: 0.951764705882353\n",
      "Validation Recall: 0.9619500594530321\n",
      "Validation F1 Score: 0.9568302779420461\n",
      "Confusion Matrix:\n",
      " [[813  29]\n",
      " [ 38 835]]\n",
      "Accuracy: 0.960932944606414\n",
      "Precision: 0.9664351851851852\n",
      "Recall: 0.9564719358533792\n",
      "F1 Score: 0.9614277489925158\n"
     ]
    }
   ],
   "source": [
    "# ======================================= METHOD 1: SVM ================================\n",
    "# Hyperparameter tuning for SVM\n",
    "svm_param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "svm_grid_search = GridSearchCV(SVC(probability=True, random_state=42), svm_param_grid, cv=5)\n",
    "svm_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters for SVM\n",
    "print(\"Best parameters for SVM:\", svm_grid_search.best_params_)\n",
    "\n",
    "# Validate and test the best SVM model\n",
    "svm_model = svm_grid_search.best_estimator_\n",
    "y_val_pred_svm = svm_model.predict(X_val)\n",
    "\n",
    "# Compute metrics for validation set\n",
    "svm_val_accuracy = accuracy_score(y_val, y_val_pred_svm)\n",
    "svm_val_precision = precision_score(y_val, y_val_pred_svm)\n",
    "svm_val_recall = recall_score(y_val, y_val_pred_svm)\n",
    "svm_val_f1_score = f1_score(y_val, y_val_pred_svm)\n",
    "\n",
    "# Print validation results\n",
    "print(\"SVM Validation Results:\")\n",
    "print(\"Validation Accuracy:\", svm_val_accuracy)\n",
    "print(\"Validation Precision:\", svm_val_precision)\n",
    "print(\"Validation Recall:\", svm_val_recall)\n",
    "print(\"Validation F1 Score:\", svm_val_f1_score)\n",
    "\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "svm_conf_matrix = confusion_matrix(y_test, y_pred_svm)\n",
    "\n",
    "# Compute metrics for SVM\n",
    "svm_tn, svm_fp, svm_fn, svm_tp = svm_conf_matrix.ravel()\n",
    "svm_accuracy = (svm_tp + svm_tn) / (svm_tp + svm_tn + svm_fp + svm_fn)\n",
    "svm_precision = svm_tp / (svm_tp + svm_fp)\n",
    "svm_recall = svm_tp / (svm_tp + svm_fn)\n",
    "svm_f1_score = (2 * (svm_precision * svm_recall)) / (svm_precision + svm_recall)\n",
    "\n",
    "# Print SVM results\n",
    "print(\"Confusion Matrix:\\n\", svm_conf_matrix)\n",
    "print(\"Accuracy:\", svm_accuracy)\n",
    "print(\"Precision:\", svm_precision)\n",
    "print(\"Recall:\", svm_recall)\n",
    "print(\"F1 Score:\", svm_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba2921f-c524-4909-b7c6-7b582df53ace",
   "metadata": {},
   "source": [
    "## Traditional Method 2: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78e90d28-a8a6-449c-9b46-a67b3756b283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Decision Tree: {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Decision Tree Validation Results:\n",
      "Validation Accuracy: 0.9323220536756126\n",
      "Validation Precision: 0.9239766081871345\n",
      "Validation Recall: 0.93935790725327\n",
      "Validation F1 Score: 0.9316037735849056\n",
      "Decision Tree Test Results:\n",
      "Confusion Matrix:\n",
      " [[796  46]\n",
      " [ 42 831]]\n",
      "Accuracy: 0.9486880466472303\n",
      "Precision: 0.9475484606613455\n",
      "Recall: 0.9518900343642611\n",
      "F1 Score: 0.9497142857142856\n"
     ]
    }
   ],
   "source": [
    "# ================================ METHOD 2: Traditional Tree ==================\n",
    "\n",
    "# Hyperparameter tuning for Decision Tree\n",
    "dt_param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "dt_grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), dt_param_grid, cv=5)\n",
    "dt_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters for Decision Tree\n",
    "print(\"Best parameters for Decision Tree:\", dt_grid_search.best_params_)\n",
    "\n",
    "# Validate and test the best Decision Tree model\n",
    "dt_model = dt_grid_search.best_estimator_\n",
    "y_val_pred_dt = dt_model.predict(X_val)\n",
    "\n",
    "# Compute metrics for validation set\n",
    "dt_val_accuracy = accuracy_score(y_val, y_val_pred_dt)\n",
    "dt_val_precision = precision_score(y_val, y_val_pred_dt)\n",
    "dt_val_recall = recall_score(y_val, y_val_pred_dt)\n",
    "dt_val_f1_score = f1_score(y_val, y_val_pred_dt)\n",
    "\n",
    "# Print validation results\n",
    "print(\"Decision Tree Validation Results:\")\n",
    "print(\"Validation Accuracy:\", dt_val_accuracy)\n",
    "print(\"Validation Precision:\", dt_val_precision)\n",
    "print(\"Validation Recall:\", dt_val_recall)\n",
    "print(\"Validation F1 Score:\", dt_val_f1_score)\n",
    "\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "dt_conf_matrix = confusion_matrix(y_test, y_pred_dt)\n",
    "\n",
    "# Compute metrics for Decision Tree\n",
    "dt_tn, dt_fp, dt_fn, dt_tp = dt_conf_matrix.ravel()\n",
    "dt_accuracy = (dt_tp + dt_tn) / (dt_tp + dt_tn + dt_fp + dt_fn)\n",
    "dt_precision = dt_tp / (dt_tp + dt_fp)\n",
    "dt_recall = dt_tp / (dt_tp + dt_fn)\n",
    "dt_f1_score = (2 * (dt_precision * dt_recall)) / (dt_precision + dt_recall)\n",
    "\n",
    "# Print Decision Tree results\n",
    "print(\"Decision Tree Test Results:\")\n",
    "print(\"Confusion Matrix:\\n\", dt_conf_matrix)\n",
    "print(\"Accuracy:\", dt_accuracy)\n",
    "print(\"Precision:\", dt_precision)\n",
    "print(\"Recall:\", dt_recall)\n",
    "print(\"F1 Score:\", dt_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bb3c64-e42b-47ce-915f-c1bfbaca011e",
   "metadata": {},
   "source": [
    "## Traditional Method 3: KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8a45576-39f8-4c60-bc7e-834fbb3c5b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 240 candidates, totalling 1200 fits\n",
      "KNeighborsClassifier(metric='manhattan', n_neighbors=16, p=1,\n",
      "                     weights='distance')\n",
      "\n",
      "Validation Results:\n",
      "Accuracy: 0.9516\n",
      "Precision: 0.9397\n",
      "Recall: 0.9631\n",
      "F1 Score: 0.9513\n",
      "Confusion Matrix:\n",
      "[[821  52]\n",
      " [ 31 810]]\n",
      "\n",
      "Final Test Results:\n",
      "Accuracy: 0.9638\n",
      "Precision: 0.9666\n",
      "Recall: 0.9622\n",
      "F1 Score: 0.9644\n",
      "Confusion Matrix:\n",
      "[[813  29]\n",
      " [ 33 840]]\n"
     ]
    }
   ],
   "source": [
    "# Define KNN model and hyperparameter grid\n",
    "knn = KNeighborsClassifier()\n",
    "param_grid = {\n",
    "    'n_neighbors': range(1, 21),           # Test k values from 1 to 20\n",
    "    'weights': ['uniform', 'distance'],    # Uniform or distance-weighted voting\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski'],  # Different distance metrics\n",
    "    'p': [1, 2]                            # Power parameter for Minkowski (p=1 is Manhattan, p=2 is Euclidean)\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV with validation set\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='recall', verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_knn = grid_search.best_estimator_\n",
    "print(best_knn)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "y_val_pred = best_knn.predict(X_val)\n",
    "y_val_pred_prob = best_knn.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Calculate validation metrics\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_precision = precision_score(y_val, y_val_pred)\n",
    "val_recall = recall_score(y_val, y_val_pred)\n",
    "val_f1 = f1_score(y_val, y_val_pred)\n",
    "val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "# Display validation results\n",
    "print(\"\\nValidation Results:\")\n",
    "print(f\"Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Precision: {val_precision:.4f}\")\n",
    "print(f\"Recall: {val_recall:.4f}\")\n",
    "print(f\"F1 Score: {val_f1:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{val_conf_matrix}\")\n",
    "\n",
    "# Final evaluation on the test set\n",
    "y_test_pred = best_knn.predict(X_test)\n",
    "y_test_pred_prob = best_knn.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate test metrics\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Display final test results\n",
    "print(\"\\nFinal Test Results:\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall: {test_recall:.4f}\")\n",
    "print(f\"F1 Score: {test_f1:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{test_conf_matrix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d34a08-0df1-44b4-95b9-404757ed2eb0",
   "metadata": {},
   "source": [
    "## Traditional Method 4: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "722c342d-1283-4459-a4ce-2a7a0eb37239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "LogisticRegression(C=100, max_iter=1000, solver='liblinear')\n",
      "\n",
      "Validation Results:\n",
      "Accuracy: 0.9347\n",
      "Precision: 0.9224\n",
      "Recall: 0.9465\n",
      "F1 Score: 0.9343\n",
      "Confusion Matrix:\n",
      "[[806  67]\n",
      " [ 45 796]]\n",
      "\n",
      "Final Test Results:\n",
      "Accuracy: 0.9388\n",
      "Precision: 0.9486\n",
      "Recall: 0.9301\n",
      "F1 Score: 0.9393\n",
      "Confusion Matrix:\n",
      "[[798  44]\n",
      " [ 61 812]]\n"
     ]
    }
   ],
   "source": [
    "# Define the Logistic Regression model and a hyperparameter grid for tuning\n",
    "logreg = LogisticRegression(max_iter=1000)  # Default Logistic Regression\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],  # Regularization parameter\n",
    "    'solver': ['liblinear', 'lbfgs']  # Solvers\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV with validation set\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='recall', verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_logreg = grid_search.best_estimator_\n",
    "print(best_logreg)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "y_val_pred = best_logreg.predict(X_val)\n",
    "y_val_pred_prob = best_logreg.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Calculate validation metrics\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_precision = precision_score(y_val, y_val_pred)\n",
    "val_recall = recall_score(y_val, y_val_pred)\n",
    "val_f1 = f1_score(y_val, y_val_pred)\n",
    "val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "# Display validation results\n",
    "print(\"\\nValidation Results:\")\n",
    "print(f\"Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Precision: {val_precision:.4f}\")\n",
    "print(f\"Recall: {val_recall:.4f}\")\n",
    "print(f\"F1 Score: {val_f1:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{val_conf_matrix}\")\n",
    "\n",
    "# Final evaluation on the test set\n",
    "y_test_pred = best_logreg.predict(X_test)\n",
    "y_test_pred_prob = best_logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate test metrics\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Display final test results\n",
    "print(\"\\nFinal Test Results:\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall: {test_recall:.4f}\")\n",
    "print(f\"F1 Score: {test_f1:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{test_conf_matrix}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f1dc19-80ed-4c7a-9991-b9b95777ed0f",
   "metadata": {},
   "source": [
    "## Traditional Method 5: Naives Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36381e8d-6b64-4a53-bd39-929140efe034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Grid Search on GaussianNB with validation set...\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "GaussianNB()\n",
      "Performing Grid Search on BernoulliNB with validation set...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "BernoulliNB(alpha=0.5)\n",
      "\n",
      "Final Test Results for GaussianNB:\n",
      "Accuracy: 0.8880\n",
      "Precision: 0.9316\n",
      "Recall: 0.8419\n",
      "F1 Score: 0.8845\n",
      "Confusion Matrix:\n",
      "[[788  54]\n",
      " [138 735]]\n",
      "\n",
      "Final Test Results for BernoulliNB:\n",
      "Accuracy: 0.9213\n",
      "Precision: 0.9222\n",
      "Recall: 0.9233\n",
      "F1 Score: 0.9227\n",
      "Confusion Matrix:\n",
      "[[774  68]\n",
      " [ 67 806]]\n"
     ]
    }
   ],
   "source": [
    "# Define the models and hyperparameter grids\n",
    "models = {\n",
    "    'GaussianNB': (GaussianNB(), {'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]}),\n",
    "    'BernoulliNB': (BernoulliNB(), {'alpha': [0.5, 1.0, 1.5, 2.0], 'binarize': [0.0, 0.5, 1.0]})\n",
    "}\n",
    "\n",
    "# Initialize results dictionary\n",
    "results = {}\n",
    "\n",
    "# Perform GridSearchCV and evaluate model on validation set function\n",
    "def grid_search_model(model, param_grid, X_train, X_val, y_train, y_val):\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='recall', verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(best_model)\n",
    "    y_val_pred = best_model.predict(X_val)\n",
    "    \n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    precision = precision_score(y_val, y_val_pred)\n",
    "    recall = recall_score(y_val, y_val_pred)\n",
    "    f1 = f1_score(y_val, y_val_pred)\n",
    "    conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "    return {\n",
    "        'best_model': best_model,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "# Perform GridSearchCV and validate each model\n",
    "for model_name, (model, param_grid) in models.items():\n",
    "    print(f\"Performing Grid Search on {model_name} with validation set...\")\n",
    "    results[model_name] = grid_search_model(model, param_grid, X_train, X_val, y_train, y_val)\n",
    "\n",
    "# Test the best models on the test set\n",
    "for model_name, metrics in results.items():\n",
    "    best_model = metrics['best_model']\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    precision = precision_score(y_test, y_test_pred)\n",
    "    recall = recall_score(y_test, y_test_pred)\n",
    "    f1 = f1_score(y_test, y_test_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "    # Display final results for the test set\n",
    "    print(f\"\\nFinal Test Results for {model_name}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f4fe42-fc93-4cfa-9346-768e9569ea44",
   "metadata": {},
   "source": [
    "# Ensemble Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94992e48-b69e-41ca-86f8-1a50ff9a9cf1",
   "metadata": {},
   "source": [
    "## Ensemble Method 1: Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfef7048-8df0-46e0-a3b0-ae9099c1d9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Random Forest Validation Results:\n",
      "Validation Accuracy: 0.9574095682613769\n",
      "Validation Precision: 0.9507042253521126\n",
      "Validation Recall: 0.9631391200951248\n",
      "Validation F1 Score: 0.9568812758417011\n",
      "Random Forest Test Results:\n",
      "Confusion Matrix:\n",
      " [[812  30]\n",
      " [ 25 848]]\n",
      "Accuracy: 0.967930029154519\n",
      "Precision: 0.9658314350797267\n",
      "Recall: 0.9713631156930126\n",
      "F1 Score: 0.9685893774985722\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning for Random Forest\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "rf_grid_search = GridSearchCV(RandomForestClassifier(random_state=42), rf_param_grid, cv=5)\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters for Random Forest\n",
    "print(\"Best parameters for Random Forest:\", rf_grid_search.best_params_)\n",
    "\n",
    "# Validate and test the best Random Forest model\n",
    "rf_model = rf_grid_search.best_estimator_\n",
    "y_val_pred_rf = rf_model.predict(X_val)\n",
    "\n",
    "# Compute metrics for validation set\n",
    "rf_val_accuracy = accuracy_score(y_val, y_val_pred_rf)\n",
    "rf_val_precision = precision_score(y_val, y_val_pred_rf)\n",
    "rf_val_recall = recall_score(y_val, y_val_pred_rf)\n",
    "rf_val_f1_score = f1_score(y_val, y_val_pred_rf)\n",
    "\n",
    "# Print validation results\n",
    "print(\"Random Forest Validation Results:\")\n",
    "print(\"Validation Accuracy:\", rf_val_accuracy)\n",
    "print(\"Validation Precision:\", rf_val_precision)\n",
    "print(\"Validation Recall:\", rf_val_recall)\n",
    "print(\"Validation F1 Score:\", rf_val_f1_score)\n",
    "\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "rf_conf_matrix = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "# Compute metrics for Random Forest\n",
    "rf_tn, rf_fp, rf_fn, rf_tp = rf_conf_matrix.ravel()\n",
    "rf_accuracy = (rf_tp + rf_tn) / (rf_tp + rf_tn + rf_fp + rf_fn)\n",
    "rf_precision = rf_tp / (rf_tp + rf_fp)\n",
    "rf_recall = rf_tp / (rf_tp + rf_fn)\n",
    "rf_f1_score = (2 * (rf_precision * rf_recall)) / (rf_precision + rf_recall)\n",
    "\n",
    "# Print Random Forest results\n",
    "print(\"Random Forest Test Results:\")\n",
    "print(\"Confusion Matrix:\\n\", rf_conf_matrix)\n",
    "print(\"Accuracy:\", rf_accuracy)\n",
    "print(\"Precision:\", rf_precision)\n",
    "print(\"Recall:\", rf_recall)\n",
    "print(\"F1 Score:\", rf_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7ad492-a5a9-4949-b43b-e24aeef89b00",
   "metadata": {},
   "source": [
    "## Ensemble Method 2: BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1155c27b-fa92-4080-b112-65bc9cd55457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Bagging Classifier: {'max_features': 0.5, 'max_samples': 0.75, 'n_estimators': 100}\n",
      "Bagging Classifier Validation Results:\n",
      "Validation Accuracy: 0.9626604434072346\n",
      "Validation Precision: 0.9628347237180526\n",
      "Validation Recall: 0.9626604434072346\n",
      "Validation F1 Score: 0.962663697465558\n",
      "Bagging Classifier Test Results:\n",
      "Confusion Matrix:\n",
      " [[810  32]\n",
      " [ 20 853]]\n",
      "Accuracy: 0.9696793002915451\n",
      "Precision: 0.9638418079096045\n",
      "Recall: 0.97709049255441\n",
      "F1 Score: 0.9704209328782707\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Define the base estimator\n",
    "base_estimator = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid for Bagging\n",
    "bagging_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_samples': [0.5, 0.75, 1.0],  # Fraction of samples to use for each base estimator\n",
    "    'max_features': [0.5, 0.75, 1.0],  # Fraction of features to use for each base estimator\n",
    "}\n",
    "\n",
    "# Create the BaggingClassifier with the base_estimator\n",
    "bagging_classifier = BaggingClassifier(random_state=42)\n",
    "\n",
    "# Perform Grid Search for Bagging Classifier\n",
    "bagging_grid_search = GridSearchCV(bagging_classifier, bagging_param_grid, cv=5)\n",
    "bagging_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters for Bagging Classifier\n",
    "print(\"Best parameters for Bagging Classifier:\", bagging_grid_search.best_params_)\n",
    "\n",
    "# Validate and test the best Bagging model\n",
    "bagging_model = bagging_grid_search.best_estimator_\n",
    "y_val_pred_bagging = bagging_model.predict(X_val)\n",
    "\n",
    "# Compute metrics for validation set\n",
    "bagging_val_accuracy = accuracy_score(y_val, y_val_pred_bagging)\n",
    "bagging_val_precision = precision_score(y_val, y_val_pred_bagging, average='weighted')\n",
    "bagging_val_recall = recall_score(y_val, y_val_pred_bagging, average='weighted')\n",
    "bagging_val_f1_score = f1_score(y_val, y_val_pred_bagging, average='weighted')\n",
    "\n",
    "# Print validation results\n",
    "print(\"Bagging Classifier Validation Results:\")\n",
    "print(\"Validation Accuracy:\", bagging_val_accuracy)\n",
    "print(\"Validation Precision:\", bagging_val_precision)\n",
    "print(\"Validation Recall:\", bagging_val_recall)\n",
    "print(\"Validation F1 Score:\", bagging_val_f1_score)\n",
    "\n",
    "# Test the best Bagging model\n",
    "y_pred_bagging = bagging_model.predict(X_test)\n",
    "bagging_conf_matrix = confusion_matrix(y_test, y_pred_bagging)\n",
    "\n",
    "# Compute metrics for Bagging Classifier\n",
    "bagging_tn, bagging_fp, bagging_fn, bagging_tp = bagging_conf_matrix.ravel()\n",
    "bagging_accuracy = (bagging_tp + bagging_tn) / (bagging_tp + bagging_tn + bagging_fp + bagging_fn)\n",
    "bagging_precision = bagging_tp / (bagging_tp + bagging_fp)\n",
    "bagging_recall = bagging_tp / (bagging_tp + bagging_fn)\n",
    "bagging_f1_score = (2 * (bagging_precision * bagging_recall)) / (bagging_precision + bagging_recall)\n",
    "\n",
    "# Print Bagging Classifier results\n",
    "print(\"Bagging Classifier Test Results:\")\n",
    "print(\"Confusion Matrix:\\n\", bagging_conf_matrix)\n",
    "print(\"Accuracy:\", bagging_accuracy)\n",
    "print(\"Precision:\", bagging_precision)\n",
    "print(\"Recall:\", bagging_recall)\n",
    "print(\"F1 Score:\", bagging_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a530c8b7-7117-4f4c-ac90-de2198a41deb",
   "metadata": {},
   "source": [
    "## Ensemble Method 3: Extra Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20585542-abee-4129-8822-9e300d1f389f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Extra Trees: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Extra Trees Validation Results:\n",
      "Validation Accuracy: 0.9579929988331388\n",
      "Validation Precision: 0.9507620164126612\n",
      "Validation Recall: 0.9643281807372176\n",
      "Validation F1 Score: 0.9574970484061394\n",
      "Extra Trees Test Results:\n",
      "Confusion Matrix:\n",
      " [[816  26]\n",
      " [ 28 845]]\n",
      "Accuracy: 0.9685131195335277\n",
      "Precision: 0.9701492537313433\n",
      "Recall: 0.9679266895761741\n",
      "F1 Score: 0.9690366972477065\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Define hyperparameter grid for Extra Trees\n",
    "et_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "# Perform Grid Search for Extra Trees\n",
    "et_grid_search = GridSearchCV(ExtraTreesClassifier(random_state=42), et_param_grid, cv=5)\n",
    "et_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters for Extra Trees\n",
    "print(\"Best parameters for Extra Trees:\", et_grid_search.best_params_)\n",
    "\n",
    "# Validate and test the best Extra Trees model\n",
    "et_model = et_grid_search.best_estimator_\n",
    "y_val_pred_et = et_model.predict(X_val)\n",
    "\n",
    "# Compute metrics for validation set\n",
    "et_val_accuracy = accuracy_score(y_val, y_val_pred_et)\n",
    "et_val_precision = precision_score(y_val, y_val_pred_et)\n",
    "et_val_recall = recall_score(y_val, y_val_pred_et)\n",
    "et_val_f1_score = f1_score(y_val, y_val_pred_et)\n",
    "\n",
    "# Print validation results\n",
    "print(\"Extra Trees Validation Results:\")\n",
    "print(\"Validation Accuracy:\", et_val_accuracy)\n",
    "print(\"Validation Precision:\", et_val_precision)\n",
    "print(\"Validation Recall:\", et_val_recall)\n",
    "print(\"Validation F1 Score:\", et_val_f1_score)\n",
    "\n",
    "y_pred_et = et_model.predict(X_test)\n",
    "et_conf_matrix = confusion_matrix(y_test, y_pred_et)\n",
    "\n",
    "# Compute metrics for Extra Trees\n",
    "et_tn, et_fp, et_fn, et_tp = et_conf_matrix.ravel()\n",
    "et_accuracy = (et_tp + et_tn) / (et_tp + et_tn + et_fp + et_fn)\n",
    "et_precision = et_tp / (et_tp + et_fp)\n",
    "et_recall = et_tp / (et_tp + et_fn)\n",
    "et_f1_score = (2 * (et_precision * et_recall)) / (et_precision + et_recall)\n",
    "\n",
    "# Print Extra Trees results\n",
    "print(\"Extra Trees Test Results:\")\n",
    "print(\"Confusion Matrix:\\n\", et_conf_matrix)\n",
    "print(\"Accuracy:\", et_accuracy)\n",
    "print(\"Precision:\", et_precision)\n",
    "print(\"Recall:\", et_recall)\n",
    "print(\"F1 Score:\", et_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5336582a-53b8-43cd-b56d-a9b80eee0578",
   "metadata": {},
   "source": [
    "## Ensemble Method: Custom of ALL TRADITIONAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4f7dff2-d38a-4396-af9e-129429de5450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Grid Search on GaussianNB with validation set...\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "GaussianNB()\n",
      "Performing Grid Search on BernoulliNB with validation set...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "BernoulliNB(alpha=0.5)\n",
      "Performing Grid Search on LogisticRegression with validation set...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "LogisticRegression(C=100, max_iter=1000, solver='liblinear')\n",
      "Performing Grid Search on DecisionTree with validation set...\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "DecisionTreeClassifier(criterion='entropy', max_depth=10)\n",
      "Performing Grid Search on SVM with validation set...\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "SVC(C=10, probability=True)\n",
      "Performing Grid Search on KNN with validation set...\n",
      "Fitting 5 folds for each of 240 candidates, totalling 1200 fits\n",
      "KNeighborsClassifier(metric='manhattan', n_neighbors=16, p=1,\n",
      "                     weights='distance')\n",
      "\n",
      "Ensemble Model Validation Results:\n",
      "Validation Accuracy: 0.9469078179696616\n",
      "Validation Precision: 0.9411764705882353\n",
      "Validation Recall: 0.9512485136741974\n",
      "Validation F1 Score: 0.9461856889414547\n",
      "Validation Confusion Matrix:\n",
      " [[823  50]\n",
      " [ 41 800]]\n",
      "\n",
      "Ensemble Model Test Results:\n",
      "Test Accuracy: 0.9626822157434403\n",
      "Test Precision: 0.9676300578034682\n",
      "Test Recall: 0.9587628865979382\n",
      "Test F1 Score: 0.9631760644418872\n",
      "Test Confusion Matrix:\n",
      " [[814  28]\n",
      " [ 36 837]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Define the models and hyperparameter grids for GridSearchCV\n",
    "models = {\n",
    "    'GaussianNB': (GaussianNB(), {'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]}),\n",
    "    'BernoulliNB': (BernoulliNB(), {'alpha': [0.5, 1.0, 1.5, 2.0], 'binarize': [0.0, 0.5, 1.0]}),\n",
    "    'LogisticRegression': (LogisticRegression(max_iter=1000), {'C': [0.1, 1, 10, 100], 'solver': ['liblinear', 'lbfgs']}),\n",
    "    'DecisionTree': (DecisionTreeClassifier(), {'criterion': ['gini', 'entropy'], 'max_depth': [None, 10, 20]}),\n",
    "    'SVM': (SVC(probability=True), {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}),\n",
    "    'KNN': (KNeighborsClassifier(), {'n_neighbors': range(1, 21), 'weights': ['uniform', 'distance'], \n",
    "                                     'metric': ['euclidean', 'manhattan', 'minkowski'], 'p': [1, 2]})\n",
    "}\n",
    "\n",
    "# Initialize results dictionary\n",
    "results = {}\n",
    "\n",
    "# Perform GridSearchCV and evaluate model on validation set function\n",
    "def grid_search_model(model, param_grid, X_train, X_val, y_train, y_val):\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='recall', verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(best_model)\n",
    "    y_val_pred = best_model.predict(X_val)\n",
    "\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    precision = precision_score(y_val, y_val_pred)\n",
    "    recall = recall_score(y_val, y_val_pred)\n",
    "    f1 = f1_score(y_val, y_val_pred)\n",
    "    conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "    return {\n",
    "        'best_model': best_model,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "# Perform GridSearchCV and validate each model\n",
    "for model_name, (model, param_grid) in models.items():\n",
    "    print(f\"Performing Grid Search on {model_name} with validation set...\")\n",
    "    results[model_name] = grid_search_model(model, param_grid, X_train, X_val, y_train, y_val)\n",
    "\n",
    "# Create a voting classifier to combine the best models\n",
    "ensemble_model = VotingClassifier(estimators=[\n",
    "    ('gaussian_nb', results['GaussianNB']['best_model']),\n",
    "    ('bernoulli_nb', results['BernoulliNB']['best_model']),\n",
    "    ('logistic_regression', results['LogisticRegression']['best_model']),\n",
    "    ('decision_tree', results['DecisionTree']['best_model']),\n",
    "    ('svm', results['SVM']['best_model']),\n",
    "    ('knn', results['KNN']['best_model'])\n",
    "], voting='soft')\n",
    "\n",
    "# Fit the ensemble model on the training data\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Validate the ensemble model on the validation set\n",
    "y_val_pred_ensemble = ensemble_model.predict(X_val)\n",
    "\n",
    "# Compute metrics for the ensemble model on the validation set\n",
    "ensemble_val_accuracy = accuracy_score(y_val, y_val_pred_ensemble)\n",
    "ensemble_val_precision = precision_score(y_val, y_val_pred_ensemble)\n",
    "ensemble_val_recall = recall_score(y_val, y_val_pred_ensemble)\n",
    "ensemble_val_f1 = f1_score(y_val, y_val_pred_ensemble)\n",
    "ensemble_val_conf_matrix = confusion_matrix(y_val, y_val_pred_ensemble)\n",
    "\n",
    "# Print validation results for the ensemble model\n",
    "print(\"\\nEnsemble Model Validation Results:\")\n",
    "print(\"Validation Accuracy:\", ensemble_val_accuracy)\n",
    "print(\"Validation Precision:\", ensemble_val_precision)\n",
    "print(\"Validation Recall:\", ensemble_val_recall)\n",
    "print(\"Validation F1 Score:\", ensemble_val_f1)\n",
    "print(\"Validation Confusion Matrix:\\n\", ensemble_val_conf_matrix)\n",
    "\n",
    "# Test the ensemble model on the test set\n",
    "y_test_pred_ensemble = ensemble_model.predict(X_test)\n",
    "\n",
    "# Compute metrics for the ensemble model on the test set\n",
    "ensemble_test_accuracy = accuracy_score(y_test, y_test_pred_ensemble)\n",
    "ensemble_test_precision = precision_score(y_test, y_test_pred_ensemble)\n",
    "ensemble_test_recall = recall_score(y_test, y_test_pred_ensemble)\n",
    "ensemble_test_f1 = f1_score(y_test, y_test_pred_ensemble)\n",
    "ensemble_test_conf_matrix = confusion_matrix(y_test, y_test_pred_ensemble)\n",
    "\n",
    "# Print ensemble model results on the test set\n",
    "print(\"\\nEnsemble Model Test Results:\")\n",
    "print(\"Test Accuracy:\", ensemble_test_accuracy)\n",
    "print(\"Test Precision:\", ensemble_test_precision)\n",
    "print(\"Test Recall:\", ensemble_test_recall)\n",
    "print(\"Test F1 Score:\", ensemble_test_f1)\n",
    "print(\"Test Confusion Matrix:\\n\", ensemble_test_conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74153548-d076-474e-a3be-3f93eab1766e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search on RandomForest\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Grid search on BaggingClassifier\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "Grid search on ExtraTrees\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "\n",
      "Bagging Ensemble Validation Results:\n",
      "Accuracy: 0.9667444574095683\n",
      "Precision: 0.96415770609319\n",
      "Recall: 0.9676258992805755\n",
      "F1 Score: 0.9658886894075404\n",
      "Confusion Matrix:\n",
      " [[850  30]\n",
      " [ 27 807]]\n",
      "\n",
      "Bagging Ensemble Test Results:\n",
      "Test Accuracy: 0.9638483965014577\n",
      "Test Precision: 0.9619377162629758\n",
      "Test Recall: 0.966396292004635\n",
      "Test F1 Score: 0.9641618497109826\n",
      "Test Confusion Matrix:\n",
      " [[819  33]\n",
      " [ 29 834]]\n"
     ]
    }
   ],
   "source": [
    "# Define bagging methods and their parameter grids\n",
    "bagging_models = {\n",
    "    'RandomForest': (RandomForestClassifier(), {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20, 30]}),\n",
    "    'BaggingClassifier': (BaggingClassifier(), {'n_estimators': [10, 20, 50], 'max_samples': [0.5, 0.75, 1.0]}),\n",
    "    'ExtraTrees': (ExtraTreesClassifier(), {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20, 30]})\n",
    "}\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Results dictionary to store each model's best parameters and metrics\n",
    "results = {}\n",
    "\n",
    "# Define function for grid search and validation\n",
    "def grid_search_model(model, param_grid, X_train, X_val, y_train, y_val):\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='recall', verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_val_pred = best_model.predict(X_val)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_val, y_val_pred),\n",
    "        'precision': precision_score(y_val, y_val_pred),\n",
    "        'recall': recall_score(y_val, y_val_pred),\n",
    "        'f1_score': f1_score(y_val, y_val_pred),\n",
    "        'confusion_matrix': confusion_matrix(y_val, y_val_pred)\n",
    "    }\n",
    "    \n",
    "    return {'best_model': best_model, 'best_params': grid_search.best_params_, **metrics}\n",
    "\n",
    "# Perform grid search for each bagging model\n",
    "for model_name, (model, param_grid) in bagging_models.items():\n",
    "    print(f\"Grid search on {model_name}\")\n",
    "    results[model_name] = grid_search_model(model, param_grid, X_train, X_val, y_train, y_val)\n",
    "\n",
    "# Create a VotingClassifier ensemble of best bagging models\n",
    "bagging_ensemble_model = VotingClassifier(estimators=[\n",
    "    ('random_forest', results['RandomForest']['best_model']),\n",
    "    ('bagging_classifier', results['BaggingClassifier']['best_model']),\n",
    "    ('extra_trees', results['ExtraTrees']['best_model'])\n",
    "], voting='soft')\n",
    "\n",
    "# Fit and validate bagging ensemble on validation set\n",
    "bagging_ensemble_model.fit(X_train, y_train)\n",
    "y_val_pred_bagging_ensemble = bagging_ensemble_model.predict(X_val)\n",
    "\n",
    "# Validation results for bagging ensemble\n",
    "bagging_ensemble_accuracy = accuracy_score(y_val, y_val_pred_bagging_ensemble)\n",
    "bagging_ensemble_precision = precision_score(y_val, y_val_pred_bagging_ensemble)\n",
    "bagging_ensemble_recall = recall_score(y_val, y_val_pred_bagging_ensemble)\n",
    "bagging_ensemble_f1 = f1_score(y_val, y_val_pred_bagging_ensemble)\n",
    "bagging_ensemble_conf_matrix = confusion_matrix(y_val, y_val_pred_bagging_ensemble)\n",
    "\n",
    "print(\"\\nBagging Ensemble Validation Results:\")\n",
    "print(\"Accuracy:\", bagging_ensemble_accuracy)\n",
    "print(\"Precision:\", bagging_ensemble_precision)\n",
    "print(\"Recall:\", bagging_ensemble_recall)\n",
    "print(\"F1 Score:\", bagging_ensemble_f1)\n",
    "print(\"Confusion Matrix:\\n\", bagging_ensemble_conf_matrix)\n",
    "\n",
    "# Test the ensemble model on the test set\n",
    "y_test_pred_bagging_ensemble = bagging_ensemble_model.predict(X_test)\n",
    "\n",
    "# Test results for bagging ensemble\n",
    "bagging_test_accuracy = accuracy_score(y_test, y_test_pred_bagging_ensemble)\n",
    "bagging_test_precision = precision_score(y_test, y_test_pred_bagging_ensemble)\n",
    "bagging_test_recall = recall_score(y_test, y_test_pred_bagging_ensemble)\n",
    "bagging_test_f1 = f1_score(y_test, y_test_pred_bagging_ensemble)\n",
    "bagging_test_conf_matrix = confusion_matrix(y_test, y_test_pred_bagging_ensemble)\n",
    "\n",
    "print(\"\\nBagging Ensemble Test Results:\")\n",
    "print(\"Test Accuracy:\", bagging_test_accuracy)\n",
    "print(\"Test Precision:\", bagging_test_precision)\n",
    "print(\"Test Recall:\", bagging_test_recall)\n",
    "print(\"Test F1 Score:\", bagging_test_f1)\n",
    "print(\"Test Confusion Matrix:\\n\", bagging_test_conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984949c8-7b22-43c2-bc4f-6b36447a5644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
