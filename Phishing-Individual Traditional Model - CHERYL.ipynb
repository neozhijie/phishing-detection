{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "887e724a-94fb-4e26-a958-3f6508bd21c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfffbf90-add9-4011-82e2-9db029fd9b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df = pd.read_csv('dataset_phishing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77671735-877f-4e9e-8c93-0e90f9420d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['shortest_word_path',\n",
    " 'ratio_intMedia',\n",
    " 'links_in_tags',\n",
    " 'nb_hyphens',\n",
    " 'page_rank',\n",
    " 'avg_word_path',\n",
    " 'ratio_extHyperlinks',\n",
    " 'longest_words_raw',\n",
    " 'google_index',\n",
    " 'length_hostname',\n",
    " 'longest_word_host',\n",
    " 'domain_registration_length',\n",
    " 'nb_www',\n",
    " 'nb_underscore',\n",
    " 'nb_dots',\n",
    " 'ratio_extMedia',\n",
    " 'phish_hints',\n",
    " 'domain_in_title',\n",
    " 'web_traffic',\n",
    " 'safe_anchor',\n",
    " 'nb_space',\n",
    " 'shortening_service',\n",
    " 'ip',\n",
    " 'domain_age',\n",
    " 'nb_qm',\n",
    " 'nb_hyperlinks',\n",
    " 'nb_slash']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c83b0ac-9b8e-4cfe-b715-4002ae2437db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[features]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df[\"status\"])\n",
    "\n",
    "# Step 1: Split data into 70% train and 30% temp (validation + test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Step 2: Split the temp set into 50% validation and 50% test (15% each of the original data)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fea61f0-d3e3-46b8-9fab-d7f5b4e00858",
   "metadata": {},
   "source": [
    "### Traditional Method 2: SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f46cfe3a-5f86-480c-9ca6-3ab1a7d27e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for SVM: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "SVM Validation Results:\n",
      "Validation Accuracy: 0.9574095682613769\n",
      "Validation Precision: 0.951764705882353\n",
      "Validation Recall: 0.9619500594530321\n",
      "Validation F1 Score: 0.9568302779420461\n",
      "Confusion Matrix:\n",
      " [[813  29]\n",
      " [ 38 835]]\n",
      "Accuracy: 0.960932944606414\n",
      "Precision: 0.9664351851851852\n",
      "Recall: 0.9564719358533792\n",
      "F1 Score: 0.9614277489925158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC  # Import SVM classifier\n",
    "\n",
    "# Hyperparameter tuning for SVM\n",
    "svm_param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "svm_grid_search = GridSearchCV(SVC(probability=True, random_state=42), svm_param_grid, cv=5)\n",
    "svm_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters for SVM\n",
    "print(\"Best parameters for SVM:\", svm_grid_search.best_params_)\n",
    "\n",
    "# Validate and test the best SVM model\n",
    "svm_model = svm_grid_search.best_estimator_\n",
    "y_val_pred_svm = svm_model.predict(X_val)\n",
    "\n",
    "# Compute metrics for validation set\n",
    "svm_val_accuracy = accuracy_score(y_val, y_val_pred_svm)\n",
    "svm_val_precision = precision_score(y_val, y_val_pred_svm)\n",
    "svm_val_recall = recall_score(y_val, y_val_pred_svm)\n",
    "svm_val_f1_score = f1_score(y_val, y_val_pred_svm)\n",
    "\n",
    "# Print validation results\n",
    "print(\"SVM Validation Results:\")\n",
    "print(\"Validation Accuracy:\", svm_val_accuracy)\n",
    "print(\"Validation Precision:\", svm_val_precision)\n",
    "print(\"Validation Recall:\", svm_val_recall)\n",
    "print(\"Validation F1 Score:\", svm_val_f1_score)\n",
    "\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "svm_conf_matrix = confusion_matrix(y_test, y_pred_svm)\n",
    "\n",
    "# Compute metrics for SVM\n",
    "svm_tn, svm_fp, svm_fn, svm_tp = svm_conf_matrix.ravel()\n",
    "svm_accuracy = (svm_tp + svm_tn) / (svm_tp + svm_tn + svm_fp + svm_fn)\n",
    "svm_precision = svm_tp / (svm_tp + svm_fp)\n",
    "svm_recall = svm_tp / (svm_tp + svm_fn)\n",
    "svm_f1_score = (2 * (svm_precision * svm_recall)) / (svm_precision + svm_recall)\n",
    "\n",
    "# Print SVM results\n",
    "print(\"Confusion Matrix:\\n\", svm_conf_matrix)\n",
    "print(\"Accuracy:\", svm_accuracy)\n",
    "print(\"Precision:\", svm_precision)\n",
    "print(\"Recall:\", svm_recall)\n",
    "print(\"F1 Score:\", svm_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20808387-3d7b-4844-bcd8-339024dd23e2",
   "metadata": {},
   "source": [
    "### Traditional Method 3: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a948c5f9-f6ef-48ec-ad01-2bb41837c36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Decision Tree: {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Decision Tree Validation Results:\n",
      "Validation Accuracy: 0.9323220536756126\n",
      "Validation Precision: 0.9239766081871345\n",
      "Validation Recall: 0.93935790725327\n",
      "Validation F1 Score: 0.9316037735849056\n",
      "Decision Tree Test Results:\n",
      "Confusion Matrix:\n",
      " [[796  46]\n",
      " [ 42 831]]\n",
      "Accuracy: 0.9486880466472303\n",
      "Precision: 0.9475484606613455\n",
      "Recall: 0.9518900343642611\n",
      "F1 Score: 0.9497142857142856\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Hyperparameter tuning for Decision Tree\n",
    "dt_param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "dt_grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), dt_param_grid, cv=5)\n",
    "dt_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters for Decision Tree\n",
    "print(\"Best parameters for Decision Tree:\", dt_grid_search.best_params_)\n",
    "\n",
    "# Validate and test the best Decision Tree model\n",
    "dt_model = dt_grid_search.best_estimator_\n",
    "y_val_pred_dt = dt_model.predict(X_val)\n",
    "\n",
    "# Compute metrics for validation set\n",
    "dt_val_accuracy = accuracy_score(y_val, y_val_pred_dt)\n",
    "dt_val_precision = precision_score(y_val, y_val_pred_dt)\n",
    "dt_val_recall = recall_score(y_val, y_val_pred_dt)\n",
    "dt_val_f1_score = f1_score(y_val, y_val_pred_dt)\n",
    "\n",
    "# Print validation results\n",
    "print(\"Decision Tree Validation Results:\")\n",
    "print(\"Validation Accuracy:\", dt_val_accuracy)\n",
    "print(\"Validation Precision:\", dt_val_precision)\n",
    "print(\"Validation Recall:\", dt_val_recall)\n",
    "print(\"Validation F1 Score:\", dt_val_f1_score)\n",
    "\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "dt_conf_matrix = confusion_matrix(y_test, y_pred_dt)\n",
    "\n",
    "# Compute metrics for Decision Tree\n",
    "dt_tn, dt_fp, dt_fn, dt_tp = dt_conf_matrix.ravel()\n",
    "dt_accuracy = (dt_tp + dt_tn) / (dt_tp + dt_tn + dt_fp + dt_fn)\n",
    "dt_precision = dt_tp / (dt_tp + dt_fp)\n",
    "dt_recall = dt_tp / (dt_tp + dt_fn)\n",
    "dt_f1_score = (2 * (dt_precision * dt_recall)) / (dt_precision + dt_recall)\n",
    "\n",
    "# Print Decision Tree results\n",
    "print(\"Decision Tree Test Results:\")\n",
    "print(\"Confusion Matrix:\\n\", dt_conf_matrix)\n",
    "print(\"Accuracy:\", dt_accuracy)\n",
    "print(\"Precision:\", dt_precision)\n",
    "print(\"Recall:\", dt_recall)\n",
    "print(\"F1 Score:\", dt_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a4bd5b-a1b4-42ad-a606-93dbbf0c73ce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ENSEMBLE METHOD 1: RANDOM FOREST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "794a3fad-c9c7-4d74-a08e-6921eb6a5612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Random Forest Validation Results:\n",
      "Validation Accuracy: 0.9574095682613769\n",
      "Validation Precision: 0.9507042253521126\n",
      "Validation Recall: 0.9631391200951248\n",
      "Validation F1 Score: 0.9568812758417011\n",
      "Random Forest Test Results:\n",
      "Confusion Matrix:\n",
      " [[812  30]\n",
      " [ 25 848]]\n",
      "Accuracy: 0.967930029154519\n",
      "Precision: 0.9658314350797267\n",
      "Recall: 0.9713631156930126\n",
      "F1 Score: 0.9685893774985722\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning for Random Forest\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "rf_grid_search = GridSearchCV(RandomForestClassifier(random_state=42), rf_param_grid, cv=5)\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters for Random Forest\n",
    "print(\"Best parameters for Random Forest:\", rf_grid_search.best_params_)\n",
    "\n",
    "# Validate and test the best Random Forest model\n",
    "rf_model = rf_grid_search.best_estimator_\n",
    "y_val_pred_rf = rf_model.predict(X_val)\n",
    "\n",
    "# Compute metrics for validation set\n",
    "rf_val_accuracy = accuracy_score(y_val, y_val_pred_rf)\n",
    "rf_val_precision = precision_score(y_val, y_val_pred_rf)\n",
    "rf_val_recall = recall_score(y_val, y_val_pred_rf)\n",
    "rf_val_f1_score = f1_score(y_val, y_val_pred_rf)\n",
    "\n",
    "# Print validation results\n",
    "print(\"Random Forest Validation Results:\")\n",
    "print(\"Validation Accuracy:\", rf_val_accuracy)\n",
    "print(\"Validation Precision:\", rf_val_precision)\n",
    "print(\"Validation Recall:\", rf_val_recall)\n",
    "print(\"Validation F1 Score:\", rf_val_f1_score)\n",
    "\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "rf_conf_matrix = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "# Compute metrics for Random Forest\n",
    "rf_tn, rf_fp, rf_fn, rf_tp = rf_conf_matrix.ravel()\n",
    "rf_accuracy = (rf_tp + rf_tn) / (rf_tp + rf_tn + rf_fp + rf_fn)\n",
    "rf_precision = rf_tp / (rf_tp + rf_fp)\n",
    "rf_recall = rf_tp / (rf_tp + rf_fn)\n",
    "rf_f1_score = (2 * (rf_precision * rf_recall)) / (rf_precision + rf_recall)\n",
    "\n",
    "# Print Random Forest results\n",
    "print(\"Random Forest Test Results:\")\n",
    "print(\"Confusion Matrix:\\n\", rf_conf_matrix)\n",
    "print(\"Accuracy:\", rf_accuracy)\n",
    "print(\"Precision:\", rf_precision)\n",
    "print(\"Recall:\", rf_recall)\n",
    "print(\"F1 Score:\", rf_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7051e630-aacc-4634-8e86-622a5f937ab5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# BAGGING: CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2129e888-e6da-4eb8-bf2b-7239d58b571f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Bagging Classifier: {'max_features': 0.5, 'max_samples': 0.75, 'n_estimators': 100}\n",
      "Bagging Classifier Validation Results:\n",
      "Validation Accuracy: 0.9626604434072346\n",
      "Validation Precision: 0.9628347237180526\n",
      "Validation Recall: 0.9626604434072346\n",
      "Validation F1 Score: 0.962663697465558\n",
      "Bagging Classifier Test Results:\n",
      "Confusion Matrix:\n",
      " [[810  32]\n",
      " [ 20 853]]\n",
      "Accuracy: 0.9696793002915451\n",
      "Precision: 0.9638418079096045\n",
      "Recall: 0.97709049255441\n",
      "F1 Score: 0.9704209328782707\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# Define the base estimator\n",
    "base_estimator = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid for Bagging\n",
    "bagging_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_samples': [0.5, 0.75, 1.0],  # Fraction of samples to use for each base estimator\n",
    "    'max_features': [0.5, 0.75, 1.0],  # Fraction of features to use for each base estimator\n",
    "}\n",
    "\n",
    "# Create the BaggingClassifier with the base_estimator\n",
    "bagging_classifier = BaggingClassifier(random_state=42)\n",
    "\n",
    "# Perform Grid Search for Bagging Classifier\n",
    "bagging_grid_search = GridSearchCV(bagging_classifier, bagging_param_grid, cv=5)\n",
    "bagging_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters for Bagging Classifier\n",
    "print(\"Best parameters for Bagging Classifier:\", bagging_grid_search.best_params_)\n",
    "\n",
    "# Validate and test the best Bagging model\n",
    "bagging_model = bagging_grid_search.best_estimator_\n",
    "y_val_pred_bagging = bagging_model.predict(X_val)\n",
    "\n",
    "# Compute metrics for validation set\n",
    "bagging_val_accuracy = accuracy_score(y_val, y_val_pred_bagging)\n",
    "bagging_val_precision = precision_score(y_val, y_val_pred_bagging, average='weighted')\n",
    "bagging_val_recall = recall_score(y_val, y_val_pred_bagging, average='weighted')\n",
    "bagging_val_f1_score = f1_score(y_val, y_val_pred_bagging, average='weighted')\n",
    "\n",
    "# Print validation results\n",
    "print(\"Bagging Classifier Validation Results:\")\n",
    "print(\"Validation Accuracy:\", bagging_val_accuracy)\n",
    "print(\"Validation Precision:\", bagging_val_precision)\n",
    "print(\"Validation Recall:\", bagging_val_recall)\n",
    "print(\"Validation F1 Score:\", bagging_val_f1_score)\n",
    "\n",
    "# Test the best Bagging model\n",
    "y_pred_bagging = bagging_model.predict(X_test)\n",
    "bagging_conf_matrix = confusion_matrix(y_test, y_pred_bagging)\n",
    "\n",
    "# Compute metrics for Bagging Classifier\n",
    "bagging_tn, bagging_fp, bagging_fn, bagging_tp = bagging_conf_matrix.ravel()\n",
    "bagging_accuracy = (bagging_tp + bagging_tn) / (bagging_tp + bagging_tn + bagging_fp + bagging_fn)\n",
    "bagging_precision = bagging_tp / (bagging_tp + bagging_fp)\n",
    "bagging_recall = bagging_tp / (bagging_tp + bagging_fn)\n",
    "bagging_f1_score = (2 * (bagging_precision * bagging_recall)) / (bagging_precision + bagging_recall)\n",
    "\n",
    "# Print Bagging Classifier results\n",
    "print(\"Bagging Classifier Test Results:\")\n",
    "print(\"Confusion Matrix:\\n\", bagging_conf_matrix)\n",
    "print(\"Accuracy:\", bagging_accuracy)\n",
    "print(\"Precision:\", bagging_precision)\n",
    "print(\"Recall:\", bagging_recall)\n",
    "print(\"F1 Score:\", bagging_f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f848b0f8-7a90-4efc-b3e5-6eaa5229743b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Bagging: Extra Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6ea0712-5a2f-457e-b509-32cc0e4d5b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Extra Trees: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Extra Trees Validation Results:\n",
      "Validation Accuracy: 0.9579929988331388\n",
      "Validation Precision: 0.9507620164126612\n",
      "Validation Recall: 0.9643281807372176\n",
      "Validation F1 Score: 0.9574970484061394\n",
      "Extra Trees Test Results:\n",
      "Confusion Matrix:\n",
      " [[816  26]\n",
      " [ 28 845]]\n",
      "Accuracy: 0.9685131195335277\n",
      "Precision: 0.9701492537313433\n",
      "Recall: 0.9679266895761741\n",
      "F1 Score: 0.9690366972477065\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Define hyperparameter grid for Extra Trees\n",
    "et_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "# Perform Grid Search for Extra Trees\n",
    "et_grid_search = GridSearchCV(ExtraTreesClassifier(random_state=42), et_param_grid, cv=5)\n",
    "et_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters for Extra Trees\n",
    "print(\"Best parameters for Extra Trees:\", et_grid_search.best_params_)\n",
    "\n",
    "# Validate and test the best Extra Trees model\n",
    "et_model = et_grid_search.best_estimator_\n",
    "y_val_pred_et = et_model.predict(X_val)\n",
    "\n",
    "# Compute metrics for validation set\n",
    "et_val_accuracy = accuracy_score(y_val, y_val_pred_et)\n",
    "et_val_precision = precision_score(y_val, y_val_pred_et)\n",
    "et_val_recall = recall_score(y_val, y_val_pred_et)\n",
    "et_val_f1_score = f1_score(y_val, y_val_pred_et)\n",
    "\n",
    "# Print validation results\n",
    "print(\"Extra Trees Validation Results:\")\n",
    "print(\"Validation Accuracy:\", et_val_accuracy)\n",
    "print(\"Validation Precision:\", et_val_precision)\n",
    "print(\"Validation Recall:\", et_val_recall)\n",
    "print(\"Validation F1 Score:\", et_val_f1_score)\n",
    "\n",
    "y_pred_et = et_model.predict(X_test)\n",
    "et_conf_matrix = confusion_matrix(y_test, y_pred_et)\n",
    "\n",
    "# Compute metrics for Extra Trees\n",
    "et_tn, et_fp, et_fn, et_tp = et_conf_matrix.ravel()\n",
    "et_accuracy = (et_tp + et_tn) / (et_tp + et_tn + et_fp + et_fn)\n",
    "et_precision = et_tp / (et_tp + et_fp)\n",
    "et_recall = et_tp / (et_tp + et_fn)\n",
    "et_f1_score = (2 * (et_precision * et_recall)) / (et_precision + et_recall)\n",
    "\n",
    "# Print Extra Trees results\n",
    "print(\"Extra Trees Test Results:\")\n",
    "print(\"Confusion Matrix:\\n\", et_conf_matrix)\n",
    "print(\"Accuracy:\", et_accuracy)\n",
    "print(\"Precision:\", et_precision)\n",
    "print(\"Recall:\", et_recall)\n",
    "print(\"F1 Score:\", et_f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa3b8a07-20f9-413e-af7d-7f59b286f457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model Validation Results:\n",
      "Validation Accuracy: 0.9434072345390898\n",
      "Validation Precision: 0.9325581395348838\n",
      "Validation Recall: 0.9536266349583828\n",
      "Validation F1 Score: 0.9429747207524986\n",
      "Ensemble Model Test Results:\n",
      "Confusion Matrix:\n",
      " [[804  38]\n",
      " [ 36 837]]\n",
      "Accuracy: 0.9568513119533528\n",
      "Precision: 0.9565714285714285\n",
      "Recall: 0.9587628865979382\n",
      "F1 Score: 0.9576659038901602\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# Assuming dt_model and svm_model are your best models from earlier code\n",
    "# Create a voting classifier to combine the Decision Tree and SVM\n",
    "ensemble_model = VotingClassifier(estimators=[\n",
    "    ('decision_tree', dt_model),\n",
    "    ('svm', svm_model)],\n",
    "    voting='soft'  # 'soft' voting uses predicted probabilities\n",
    ")\n",
    "\n",
    "# Fit the ensemble model on the training data\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Validate and test the ensemble model\n",
    "y_val_pred_ensemble = ensemble_model.predict(X_val)\n",
    "\n",
    "# Compute metrics for validation set\n",
    "ensemble_val_accuracy = accuracy_score(y_val, y_val_pred_ensemble)\n",
    "ensemble_val_precision = precision_score(y_val, y_val_pred_ensemble)\n",
    "ensemble_val_recall = recall_score(y_val, y_val_pred_ensemble)\n",
    "ensemble_val_f1_score = f1_score(y_val, y_val_pred_ensemble)\n",
    "\n",
    "# Print validation results for the ensemble model\n",
    "print(\"Ensemble Model Validation Results:\")\n",
    "print(\"Validation Accuracy:\", ensemble_val_accuracy)\n",
    "print(\"Validation Precision:\", ensemble_val_precision)\n",
    "print(\"Validation Recall:\", ensemble_val_recall)\n",
    "print(\"Validation F1 Score:\", ensemble_val_f1_score)\n",
    "\n",
    "# Test the ensemble model\n",
    "y_pred_ensemble = ensemble_model.predict(X_test)\n",
    "ensemble_conf_matrix = confusion_matrix(y_test, y_pred_ensemble)\n",
    "\n",
    "# Compute metrics for the ensemble model\n",
    "ensemble_tn, ensemble_fp, ensemble_fn, ensemble_tp = ensemble_conf_matrix.ravel()\n",
    "ensemble_accuracy = (ensemble_tp + ensemble_tn) / (ensemble_tp + ensemble_tn + ensemble_fp + ensemble_fn)\n",
    "ensemble_precision = ensemble_tp / (ensemble_tp + ensemble_fp)\n",
    "ensemble_recall = ensemble_tp / (ensemble_tp + ensemble_fn)\n",
    "ensemble_f1_score = (2 * (ensemble_precision * ensemble_recall)) / (ensemble_precision + ensemble_recall)\n",
    "\n",
    "# Print ensemble model results\n",
    "print(\"Ensemble Model Test Results:\")\n",
    "print(\"Confusion Matrix:\\n\", ensemble_conf_matrix)\n",
    "print(\"Accuracy:\", ensemble_accuracy)\n",
    "print(\"Precision:\", ensemble_precision)\n",
    "print(\"Recall:\", ensemble_recall)\n",
    "print(\"F1 Score:\", ensemble_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a321e30-4893-4e27-b76a-db741ad06a47",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#  KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19a45fb4-effa-43c2-bb69-0cc71491e1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 240 candidates, totalling 1200 fits\n",
      "\n",
      "Validation Results:\n",
      "Accuracy: 0.9516\n",
      "Precision: 0.9397\n",
      "Recall: 0.9631\n",
      "F1 Score: 0.9513\n",
      "ROC-AUC Score: 0.9890\n",
      "Confusion Matrix:\n",
      "[[821  52]\n",
      " [ 31 810]]\n",
      "\n",
      "Final Test Results:\n",
      "Accuracy: 0.9638\n",
      "Precision: 0.9666\n",
      "Recall: 0.9622\n",
      "F1 Score: 0.9644\n",
      "ROC-AUC Score: 0.9899\n",
      "Confusion Matrix:\n",
      "[[813  29]\n",
      " [ 33 840]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix)\n",
    "\n",
    "# Define KNN model and hyperparameter grid\n",
    "knn = KNeighborsClassifier()\n",
    "param_grid = {\n",
    "    'n_neighbors': range(1, 21),           # Test k values from 1 to 20\n",
    "    'weights': ['uniform', 'distance'],    # Uniform or distance-weighted voting\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski'],  # Different distance metrics\n",
    "    'p': [1, 2]                            # Power parameter for Minkowski (p=1 is Manhattan, p=2 is Euclidean)\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV with validation set\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='recall', verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_knn = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate on the validation set\n",
    "y_val_pred = best_knn.predict(X_val)\n",
    "y_val_pred_prob = best_knn.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Calculate validation metrics\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_precision = precision_score(y_val, y_val_pred)\n",
    "val_recall = recall_score(y_val, y_val_pred)\n",
    "val_f1 = f1_score(y_val, y_val_pred)\n",
    "val_roc_auc = roc_auc_score(y_val, y_val_pred_prob)\n",
    "val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "# Display validation results\n",
    "print(\"\\nValidation Results:\")\n",
    "print(f\"Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Precision: {val_precision:.4f}\")\n",
    "print(f\"Recall: {val_recall:.4f}\")\n",
    "print(f\"F1 Score: {val_f1:.4f}\")\n",
    "print(f\"ROC-AUC Score: {val_roc_auc:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{val_conf_matrix}\")\n",
    "\n",
    "# Final evaluation on the test set\n",
    "y_test_pred = best_knn.predict(X_test)\n",
    "y_test_pred_prob = best_knn.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate test metrics\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "test_roc_auc = roc_auc_score(y_test, y_test_pred_prob)\n",
    "test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Display final test results\n",
    "print(\"\\nFinal Test Results:\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall: {test_recall:.4f}\")\n",
    "print(f\"F1 Score: {test_f1:.4f}\")\n",
    "print(f\"ROC-AUC Score: {test_roc_auc:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{test_conf_matrix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fff0df-a739-429d-b6ba-89b47a008bc8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3767470b-a9aa-4ee4-a73b-f04f91f0008b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "\n",
      "Validation Results:\n",
      "Accuracy: 0.9347\n",
      "Precision: 0.9224\n",
      "Recall: 0.9465\n",
      "F1 Score: 0.9343\n",
      "ROC-AUC Score: 0.9819\n",
      "Confusion Matrix:\n",
      "[[806  67]\n",
      " [ 45 796]]\n",
      "\n",
      "Final Test Results:\n",
      "Accuracy: 0.9388\n",
      "Precision: 0.9486\n",
      "Recall: 0.9301\n",
      "F1 Score: 0.9393\n",
      "ROC-AUC Score: 0.9847\n",
      "Confusion Matrix:\n",
      "[[798  44]\n",
      " [ 61 812]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix)\n",
    "\n",
    "# Define the Logistic Regression model and a hyperparameter grid for tuning\n",
    "logreg = LogisticRegression(max_iter=1000)  # Default Logistic Regression\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],  # Regularization parameter\n",
    "    'solver': ['liblinear', 'lbfgs']  # Solvers\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV with validation set\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='recall', verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_logreg = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate on the validation set\n",
    "y_val_pred = best_logreg.predict(X_val)\n",
    "y_val_pred_prob = best_logreg.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Calculate validation metrics\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_precision = precision_score(y_val, y_val_pred)\n",
    "val_recall = recall_score(y_val, y_val_pred)\n",
    "val_f1 = f1_score(y_val, y_val_pred)\n",
    "val_roc_auc = roc_auc_score(y_val, y_val_pred_prob)\n",
    "val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "# Display validation results\n",
    "print(\"\\nValidation Results:\")\n",
    "print(f\"Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Precision: {val_precision:.4f}\")\n",
    "print(f\"Recall: {val_recall:.4f}\")\n",
    "print(f\"F1 Score: {val_f1:.4f}\")\n",
    "print(f\"ROC-AUC Score: {val_roc_auc:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{val_conf_matrix}\")\n",
    "\n",
    "# Final evaluation on the test set\n",
    "y_test_pred = best_logreg.predict(X_test)\n",
    "y_test_pred_prob = best_logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate test metrics\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "test_roc_auc = roc_auc_score(y_test, y_test_pred_prob)\n",
    "test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Display final test results\n",
    "print(\"\\nFinal Test Results:\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall: {test_recall:.4f}\")\n",
    "print(f\"F1 Score: {test_f1:.4f}\")\n",
    "print(f\"ROC-AUC Score: {test_roc_auc:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{test_conf_matrix}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd814c7-b44e-4757-96ad-b814e404b440",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Naives Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb4837c0-7e6b-488a-94c3-9afe6f90e83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Grid Search on GaussianNB with validation set...\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Performing Grid Search on BernoulliNB with validation set...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "\n",
      "Final Test Results for GaussianNB:\n",
      "Accuracy: 0.8880\n",
      "Precision: 0.9316\n",
      "Recall: 0.8419\n",
      "F1 Score: 0.8845\n",
      "ROC-AUC Score: 0.9544\n",
      "Confusion Matrix:\n",
      "[[788  54]\n",
      " [138 735]]\n",
      "\n",
      "Final Test Results for BernoulliNB:\n",
      "Accuracy: 0.9213\n",
      "Precision: 0.9222\n",
      "Recall: 0.9233\n",
      "F1 Score: 0.9227\n",
      "ROC-AUC Score: 0.9692\n",
      "Confusion Matrix:\n",
      "[[774  68]\n",
      " [ 67 806]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Define the models and hyperparameter grids\n",
    "models = {\n",
    "    'GaussianNB': (GaussianNB(), {'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]}),\n",
    "    'BernoulliNB': (BernoulliNB(), {'alpha': [0.5, 1.0, 1.5, 2.0], 'binarize': [0.0, 0.5, 1.0]})\n",
    "}\n",
    "\n",
    "# Initialize results dictionary\n",
    "results = {}\n",
    "\n",
    "# Perform GridSearchCV and evaluate model on validation set function\n",
    "def grid_search_model(model, param_grid, X_train, X_val, y_train, y_val):\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='recall', verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_val_pred = best_model.predict(X_val)\n",
    "    y_val_pred_prob = best_model.predict_proba(X_val)[:, 1]  # For ROC-AUC score\n",
    "\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    precision = precision_score(y_val, y_val_pred)\n",
    "    recall = recall_score(y_val, y_val_pred)\n",
    "    f1 = f1_score(y_val, y_val_pred)\n",
    "    roc_auc = roc_auc_score(y_val, y_val_pred_prob)\n",
    "    conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "    return {\n",
    "        'best_model': best_model,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "# Perform GridSearchCV and validate each model\n",
    "for model_name, (model, param_grid) in models.items():\n",
    "    print(f\"Performing Grid Search on {model_name} with validation set...\")\n",
    "    results[model_name] = grid_search_model(model, param_grid, X_train, X_val, y_train, y_val)\n",
    "\n",
    "# Test the best models on the test set\n",
    "for model_name, metrics in results.items():\n",
    "    best_model = metrics['best_model']\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    y_test_pred_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    precision = precision_score(y_test, y_test_pred)\n",
    "    recall = recall_score(y_test, y_test_pred)\n",
    "    f1 = f1_score(y_test, y_test_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_test_pred_prob)\n",
    "    conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "    # Display final results for the test set\n",
    "    print(f\"\\nFinal Test Results for {model_name}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6336ae2-91c9-4f16-9cf9-afd0a95e6c0f",
   "metadata": {},
   "source": [
    "# Combine SVM, Decision Tree using Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c27e01d0-9f7d-4ec7-959e-85255e5fb557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Grid Search on GaussianNB with validation set...\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Performing Grid Search on BernoulliNB with validation set...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Performing Grid Search on LogisticRegression with validation set...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Performing Grid Search on DecisionTree with validation set...\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Performing Grid Search on SVM with validation set...\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "\n",
      "Ensemble Model Validation Results:\n",
      "Validation Accuracy: 0.9439906651108518\n",
      "Validation Precision: 0.9377203290246768\n",
      "Validation Recall: 0.9488703923900119\n",
      "Validation F1 Score: 0.9432624113475178\n",
      "Validation Confusion Matrix:\n",
      " [[820  53]\n",
      " [ 43 798]]\n",
      "\n",
      "Final Test Results for GaussianNB:\n",
      "Accuracy: 0.8880\n",
      "Precision: 0.9316\n",
      "Recall: 0.8419\n",
      "F1 Score: 0.8845\n",
      "Confusion Matrix:\n",
      "[[788  54]\n",
      " [138 735]]\n",
      "\n",
      "Final Test Results for BernoulliNB:\n",
      "Accuracy: 0.9213\n",
      "Precision: 0.9222\n",
      "Recall: 0.9233\n",
      "F1 Score: 0.9227\n",
      "Confusion Matrix:\n",
      "[[774  68]\n",
      " [ 67 806]]\n",
      "\n",
      "Final Test Results for LogisticRegression:\n",
      "Accuracy: 0.9388\n",
      "Precision: 0.9486\n",
      "Recall: 0.9301\n",
      "F1 Score: 0.9393\n",
      "Confusion Matrix:\n",
      "[[798  44]\n",
      " [ 61 812]]\n",
      "\n",
      "Final Test Results for DecisionTree:\n",
      "Accuracy: 0.9423\n",
      "Precision: 0.9428\n",
      "Recall: 0.9439\n",
      "F1 Score: 0.9433\n",
      "Confusion Matrix:\n",
      "[[792  50]\n",
      " [ 49 824]]\n",
      "\n",
      "Final Test Results for SVM:\n",
      "Accuracy: 0.9609\n",
      "Precision: 0.9664\n",
      "Recall: 0.9565\n",
      "F1 Score: 0.9614\n",
      "Confusion Matrix:\n",
      "[[813  29]\n",
      " [ 38 835]]\n",
      "\n",
      "Ensemble Model Test Results:\n",
      "Test Accuracy: 0.956268221574344\n",
      "Test Precision: 0.9607390300230947\n",
      "Test Recall: 0.9530355097365406\n",
      "Test F1 Score: 0.9568717653824037\n",
      "Test Confusion Matrix: [[808  34]\n",
      " [ 41 832]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.svm import SVC  # Assuming you have SVC for the ensemble\n",
    "from sklearn.tree import DecisionTreeClassifier  # Assuming you have Decision Tree\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix)\n",
    "import joblib\n",
    "\n",
    "# Define the models and hyperparameter grids for GridSearchCV\n",
    "models = {\n",
    "    'GaussianNB': (GaussianNB(), {'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]}),\n",
    "    'BernoulliNB': (BernoulliNB(), {'alpha': [0.5, 1.0, 1.5, 2.0], 'binarize': [0.0, 0.5, 1.0]}),\n",
    "    'LogisticRegression': (LogisticRegression(max_iter=1000), {'C': [0.1, 1, 10, 100], 'solver': ['liblinear', 'lbfgs']}),\n",
    "    'DecisionTree': (DecisionTreeClassifier(), {}),  # Add hyperparameters if needed\n",
    "    'SVM': (SVC(probability=True), {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']})\n",
    "}\n",
    "\n",
    "# Initialize results dictionary\n",
    "results = {}\n",
    "\n",
    "# Perform GridSearchCV and evaluate model on validation set function\n",
    "def grid_search_model(model, param_grid, X_train, X_val, y_train, y_val):\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='recall', verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_val_pred = best_model.predict(X_val)\n",
    "    y_val_pred_prob = best_model.predict_proba(X_val)[:, 1]  # For ROC-AUC score\n",
    "\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    precision = precision_score(y_val, y_val_pred)\n",
    "    recall = recall_score(y_val, y_val_pred)\n",
    "    f1 = f1_score(y_val, y_val_pred)\n",
    "    roc_auc = roc_auc_score(y_val, y_val_pred_prob)\n",
    "    conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "    return {\n",
    "        'best_model': best_model,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "# Perform GridSearchCV and validate each model\n",
    "for model_name, (model, param_grid) in models.items():\n",
    "    print(f\"Performing Grid Search on {model_name} with validation set...\")\n",
    "    results[model_name] = grid_search_model(model, param_grid, X_train, X_val, y_train, y_val)\n",
    "\n",
    "# Create a voting classifier to combine the best models\n",
    "ensemble_model = VotingClassifier(estimators=[\n",
    "    ('gaussian_nb', results['GaussianNB']['best_model']),\n",
    "    ('bernoulli_nb', results['BernoulliNB']['best_model']),\n",
    "    ('logistic_regression', results['LogisticRegression']['best_model']),\n",
    "    ('decision_tree', results['DecisionTree']['best_model']),\n",
    "    ('svm', results['SVM']['best_model'])\n",
    "], voting='soft')  # 'soft' voting uses predicted probabilities\n",
    "\n",
    "# Fit the ensemble model on the training data\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Validate the ensemble model on the validation set\n",
    "y_val_pred_ensemble = ensemble_model.predict(X_val)\n",
    "y_val_pred_prob_ensemble = ensemble_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Compute metrics for the validation set\n",
    "ensemble_val_accuracy = accuracy_score(y_val, y_val_pred_ensemble)\n",
    "ensemble_val_precision = precision_score(y_val, y_val_pred_ensemble)\n",
    "ensemble_val_recall = recall_score(y_val, y_val_pred_ensemble)\n",
    "ensemble_val_f1 = f1_score(y_val, y_val_pred_ensemble)\n",
    "ensemble_val_conf_matrix = confusion_matrix(y_val, y_val_pred_ensemble)\n",
    "\n",
    "# Print validation results for the ensemble model\n",
    "print(\"\\nEnsemble Model Validation Results:\")\n",
    "print(\"Validation Accuracy:\", ensemble_val_accuracy)\n",
    "print(\"Validation Precision:\", ensemble_val_precision)\n",
    "print(\"Validation Recall:\", ensemble_val_recall)\n",
    "print(\"Validation F1 Score:\", ensemble_val_f1)\n",
    "print(\"Validation Confusion Matrix:\\n\", ensemble_val_conf_matrix)\n",
    "\n",
    "# Test the ensemble model on the test set\n",
    "for model_name, metrics in results.items():\n",
    "    best_model = metrics['best_model']\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    y_test_pred_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    precision = precision_score(y_test, y_test_pred)\n",
    "    recall = recall_score(y_test, y_test_pred)\n",
    "    f1 = f1_score(y_test, y_test_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "    print(f\"\\nFinal Test Results for {model_name}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "\n",
    "\n",
    "# Test the ensemble model on the test set\n",
    "y_test_pred_ensemble = ensemble_model.predict(X_test)\n",
    "y_test_pred_prob_ensemble = ensemble_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute metrics for the ensemble model on the test set\n",
    "ensemble_test_accuracy = accuracy_score(y_test, y_test_pred_ensemble)\n",
    "ensemble_test_precision = precision_score(y_test, y_test_pred_ensemble)\n",
    "ensemble_test_recall = recall_score(y_test, y_test_pred_ensemble)\n",
    "ensemble_test_f1 = f1_score(y_test, y_test_pred_ensemble)\n",
    "ensemble_test_conf_matrix = confusion_matrix(y_test, y_test_pred_ensemble)\n",
    "\n",
    "# Print ensemble model results on the test set\n",
    "print(\"\\nEnsemble Model Test Results:\")\n",
    "print(\"Test Accuracy:\", ensemble_test_accuracy)\n",
    "print(\"Test Precision:\", ensemble_test_precision)\n",
    "print(\"Test Recall:\", ensemble_test_recall)\n",
    "print(\"Test F1 Score:\", ensemble_test_f1)\n",
    "print(\"Test Confusion Matrix:\", ensemble_test_conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b01f20c-7402-4ce4-94c0-c8c3e36fe699",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
